{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬 표준 라이브러리\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from concurrent import futures\n",
    "\n",
    "# 파이썬 서드파티 라이브러리\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전역 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# cpu 갯수\n",
    "workers = os.cpu_count()\n",
    "print(workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_websites_dict = {\n",
    "                    'investing': 'https://kr.investing.com/news/cryptocurrency-news',\n",
    "                    'hankyung': 'https://www.hankyung.com/koreamarket/news/crypto',\n",
    "                    'bloomingbit': 'https://bloomingbit.io/feed',\n",
    "                    'coinreaders': 'https://www.coinreaders.com/',\n",
    "                    'cryptonews': 'https://cryptonews.com/kr/news/',\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chrome 옵션 설정\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# User-Agent 변경을 위한 옵션 설정\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "headers = {'User-Agent': user_agent}\n",
    "options.add_argument(f\"user-agent={user_agent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hankyung(news_url: bs4.element.Tag) -> dict[str, str, None]:\n",
    "    \"\"\"뉴스 URL을 바탕으로 크롤링을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        news_url: 뉴스 URL\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"news_title\": 뉴스 제목, str\n",
    "            \"news_first_upload_time\": 뉴스 최초 업로드 시각, str | None\n",
    "            \"newsfinal_upload_time\": 뉴스 최종 수정 시각, str\n",
    "            \"author\": 뉴스 작성자\n",
    "            \"news_content\": 뉴스 본문, str\n",
    "            \"news_url\": 뉴스 URL, str\n",
    "            \"news_website\": 뉴스 웹사이트, str\n",
    "        }\n",
    "    \"\"\"\n",
    "    info = {} # 뉴스 데이터 정보 Dictionary\n",
    "\n",
    "    # news_url를 찾아서 requests로 HTML GET을 한 다음, BeautifulSoup로 parser\n",
    "    url = news_url.find(\"a\")[\"href\"]\n",
    "    html = requests.get(url).text\n",
    "    time.sleep(0.1)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 1. 뉴스 데이터의 제목\n",
    "    title = soup.find('h1', class_='headline').text\n",
    "\n",
    "    # 2. 뉴스 데이터의 최초 업로드 시각과 최종 수정 시각\n",
    "    first_upload_time = None\n",
    "    div = soup.find_all('span', class_='txt-date')\n",
    "    span = div[1].find('span')\n",
    "    last_upload_time_list = re.split(r'\\s+\\r\\n\\s+', span.text)[1].split()\n",
    "    y_m_d = '-'.join(times[:-1] for times in last_upload_time_list[:3])\n",
    "    if last_upload_time_list[3] == '오전':\n",
    "        ap = 'AM'\n",
    "    else:\n",
    "        ap = 'PM'\n",
    "    last_upload_time = y_m_d + ' ' + ap + ' ' + last_upload_time_list[4]\n",
    "\n",
    "    # 3. 뉴스 데이터의 기사 작성자\n",
    "    author = None\n",
    "\n",
    "    # 4. 뉴스 데이터의 본문\n",
    "    div = soup.find('div', id='article')\n",
    "    news_content = div.find_all('p')\n",
    "\n",
    "    # 뉴스 데이터 본문의 데이터 전처리1\n",
    "    if re.search('(읽기|provided|[a-z0-9]@[a-z0-9])', news_content[-1].text, flags=re.IGNORECASE):\n",
    "        del news_content[-1]\n",
    "    # 뉴스 데이터 본문의 데이터 전처리2\n",
    "    text = ''.join(t.text for t in news_content).strip(r' \\t\\n\\r\\f\\v')\n",
    "    text = re.sub(r'([가-힣])\\.(\\w)', r'\\g<1>. \\g<2>', text)\n",
    "    text = re.sub('(\\xa0)+', ' ', text)\n",
    "\n",
    "    info['news_title'] = title\n",
    "    info['news_first_upload_time'] = first_upload_time\n",
    "    info['news_last_upload_time'] = last_upload_time\n",
    "    info['author'] = author\n",
    "    info['news_content'] = text\n",
    "    info['news_url'] = url\n",
    "    info['news_website'] = 'Investing'\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05 AM 07:51\n",
      "2024-12-05 AM 07:51\n"
     ]
    }
   ],
   "source": [
    "web_page = 'https://www.hankyung.com/koreamarket/news/crypto'\n",
    "results = []\n",
    "\n",
    "html = requests.get(web_page).text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "url_tag_list = soup.find_all('h2', {\"class\": \"news-tit\"})\n",
    "info = {} # 뉴스 데이터 정보 Dictionary\n",
    "\n",
    "# news_url를 찾아서 requests로 HTML GET을 한 다음, BeautifulSoup로 parser\n",
    "for news_url in url_tag_list:\n",
    "    url = news_url.find(\"a\")[\"href\"]\n",
    "    html = requests.get(url).text\n",
    "    time.sleep(0.1)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 1. 뉴스 데이터의 제목\n",
    "    title = soup.find('h1', class_='headline').text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 2. 뉴스 데이터의 최초 업로드 시각과 최종 수정 시각\n",
    "    first_upload_time = None\n",
    "    span = soup.find_all('span', class_='txt-date')\n",
    "    first_upload_time = span[0].text\n",
    "    last_upload_time = span[1].text\n",
    "\n",
    "    first_upload_time = datetime.strptime(first_upload_time, '%Y.%m.%d %H:%M')\n",
    "    first_upload_time = datetime.strftime(first_upload_time, '%Y-%m-%d %p %H:%M')\n",
    "    last_upload_time = datetime.strptime(last_upload_time, '%Y.%m.%d %H:%M')\n",
    "    last_upload_time = datetime.strftime(last_upload_time, '%Y-%m-%d %p %H:%M')\n",
    "\n",
    "    # # 3. 뉴스 데이터의 기사 작성자\n",
    "    author = None\n",
    "\n",
    "    # # 4. 뉴스 데이터의 본문\n",
    "    # div = soup.find('div', id='article')\n",
    "    # news_content = div.find_all('p')\n",
    "\n",
    "    # # 뉴스 데이터 본문의 데이터 전처리1\n",
    "    # if re.search('(읽기|provided|[a-z0-9]@[a-z0-9])', news_content[-1].text, flags=re.IGNORECASE):\n",
    "    #     del news_content[-1]\n",
    "    # # 뉴스 데이터 본문의 데이터 전처리2\n",
    "    # text = ''.join(t.text for t in news_content).strip(r' \\t\\n\\r\\f\\v')\n",
    "    # text = re.sub(r'([가-힣])\\.(\\w)', r'\\g<1>. \\g<2>', text)\n",
    "    # text = re.sub('(\\xa0)+', ' ', text)\n",
    "\n",
    "    # info['news_title'] = title\n",
    "    # info['news_first_upload_time'] = first_upload_time\n",
    "    # info['news_last_upload_time'] = last_upload_time\n",
    "    # info['author'] = author\n",
    "    # info['news_content'] = text\n",
    "    # info['news_url'] = url\n",
    "    # info['news_website'] = 'Hankyung'\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_page = 'https://www.hankyung.com/koreamarket/news/crypto'\n",
    "results = []\n",
    "\n",
    "for i in range(2, 2001):\n",
    "    html = requests.get(web_page).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    url_tag_list = soup.find_all('h2', {\"class\": \"news-tit\"})\n",
    "\n",
    "    try:\n",
    "        for url_tag in url_tag_list:\n",
    "            data = hankyung(url_tag)\n",
    "            results.append(data)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    time.sleep(random.uniform(0.25, 1))\n",
    "    web_page = f'https://www.hankyung.com/koreamarket/news/crypto?page={i}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
