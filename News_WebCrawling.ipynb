{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14345,
     "status": "ok",
     "timestamp": 1733586568195,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "9HokvFOG2ICx",
    "outputId": "d9c303a8-dc89-45cc-941a-e5b735d02dcd"
   },
   "outputs": [],
   "source": [
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10814,
     "status": "ok",
     "timestamp": 1733586579007,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "oKtWa7v02ICx",
    "outputId": "010a47b4-bff7-4ac7-e294-59fc0f062de2"
   },
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6792,
     "status": "ok",
     "timestamp": 1733586585797,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "2Zf1EfhFiiH9",
    "outputId": "6c49166f-fa78-4ce8-be90-231ebe539291"
   },
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6689,
     "status": "ok",
     "timestamp": 1733586592483,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "QTKX8KU0isEP",
    "outputId": "151ddd56-2e75-4602-b25e-9428e1d4ef47"
   },
   "outputs": [],
   "source": [
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcXucBjY20Ci"
   },
   "source": [
    "# 문제점\n",
    "\n",
    "1. 암호화폐 뉴스 사이트들 대부분이 서로 크롤링 하는 경우가 많아서 이미 해당 사건으로부터 몇 십분에서 몇 시간, 며칠이 지난 후에 뉴스가 올라오는 경우가 되게 많음\n",
    "\n",
    "예: A사이트는 B사이트를 크롤링해서 뉴스 게재 -> B사이트는 C사이트를 크롤링해서 뉴스 게재 -> C사이트는 A사이트를 크롤링해서 게재\n",
    "\n",
    "2. 원인 불명의 문제로 인해 크롤링할 때 일부 뉴스 데이터들이 누락"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4H1DYrC82ICy"
   },
   "source": [
    "# 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1733665110553,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "auvaotrw2ICz"
   },
   "outputs": [],
   "source": [
    "# 파이썬 표준 라이브러리\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from concurrent import futures\n",
    "\n",
    "# 파이썬 서드파티 라이브러리\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxDIDmyG2IDA"
   },
   "source": [
    "# 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_html(\n",
    "                url: str, headers: dict[str, str], allow_redirects: bool, timeout: int,\n",
    "                 max_retry: int\n",
    "                 ) -> str | None:\n",
    "    \"\"\"requests로 HTML 문서 정보를 불러오는 함수\n",
    "\n",
    "    Args:\n",
    "        url: 뉴스 URL\n",
    "        headers: 식별 정보\n",
    "        allow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "    \n",
    "    Return:\n",
    "        텍스트화한 HTML 문서 정보, str\n",
    "\n",
    "        or \n",
    "    \n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    html = None\n",
    "\n",
    "    for _ in range(max_retry):\n",
    "        # requests로 HTML GET\n",
    "        response = requests.get(url, headers=headers, allow_redirects=allow_redirects, timeout=timeout)\n",
    "        # HTML 문서 정보를 불러오는 것에 성공하면 for문 중단\n",
    "        if response.status_code == requests.codes.ok:\n",
    "            html = response.text\n",
    "            break\n",
    "        # 실패한 응답 요청을 출력\n",
    "        try:\n",
    "            response.raise_for_status()\n",
    "        except Exception:\n",
    "            print()\n",
    "            print(f'HTML 문서 정보 가져오기를 실패한 뉴스 URL : {url}')\n",
    "            print(traceback.format_exc())\n",
    "        time.sleep(random.uniform(0.35, 1.25))\n",
    "    \n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1733669946448,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "jdcV0NWCW0rV"
   },
   "outputs": [],
   "source": [
    "def investing(\n",
    "            news_url: str, headers: dict[str, str], allow_redirects: bool, timeout: int, max_retry: int,\n",
    "              p0: re.Pattern, p1: re.Pattern, p2: re.Pattern, p3: re.Pattern\n",
    "              ) -> dict[str, str, None] | None:\n",
    "    \"\"\"뉴스 URL을 바탕으로 크롤링을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        news_url_tag: 뉴스 URL\n",
    "        headers: 식별 정보\n",
    "        allow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        p0: \"입력: \\r\\n 2024- 12- 07- 오후 08:45\"와같은 텍스트에서 ' \\r\\n '를 기준으로 분리하는 패턴\n",
    "        p1: 디셉터에서 읽기 / Provided COINNESS / 이승훈 기자 123@gmail.com 등의 불필요한 텍스트 패턴\n",
    "        p2: \"발표했습니다.그리고\"와 같이 마침표 다음에 유니코드 문자열이 바로 붙어있는 텍스트 패턴\n",
    "        p3: \"\\xa0\"가 1개 이상 연달아 붙어있는 텍스트 패턴\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"news_title\": 뉴스 제목, str\n",
    "            \"news_first_upload_time\": 뉴스 최초 업로드 시각, str | None\n",
    "            \"newsfinal_upload_time\": 뉴스 최종 수정 시각, str | None\n",
    "            \"author\": 뉴스 작성자, str | None\n",
    "            \"news_content\": 뉴스 본문, str\n",
    "            \"news_url\": 뉴스 URL, str\n",
    "            \"news_website\": 뉴스 웹사이트, str\n",
    "            \"note\": 비고, str | None\n",
    "        }\n",
    "\n",
    "        or\n",
    "\n",
    "        None\n",
    "    \"\"\"\n",
    "    info = {} # 뉴스 데이터 정보 Dictionary\n",
    "\n",
    "    # requests로 HTML GET\n",
    "    html = request_html(url=news_url, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "    # HTML 문서 정보를 불러오는 것에 실패하면 None 반환\n",
    "    if html is None:\n",
    "        return None\n",
    "    # BeautifulSoup로 parser\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 1. 뉴스 데이터의 제목\n",
    "    title = soup.find('h1', id='articleTitle')\n",
    "    if title is None:\n",
    "        title = None\n",
    "    else:\n",
    "        title = title.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 2. 뉴스 데이터의 최초 업로드 시각과 최종 수정 시각\n",
    "    div = soup.find_all('div', {'class': 'flex flex-row items-center'})\n",
    "    if div:\n",
    "        span = div[1].find('span')\n",
    "        if span is None:\n",
    "            first_upload_time = None\n",
    "        else:\n",
    "            span = span.text\n",
    "            first_upload_time_list = p0.split(span)[1].split()\n",
    "            y_m_d = '-'.join(times[:-1] for times in first_upload_time_list[:3])\n",
    "            if first_upload_time_list[3] == '오전':\n",
    "                ap = 'AM'\n",
    "            else:\n",
    "                ap = 'PM'\n",
    "\n",
    "            first_upload_time = y_m_d + ' ' + ap + ' ' + first_upload_time_list[4]\n",
    "        last_upload_time = None\n",
    "    else:\n",
    "        first_upload_time = None\n",
    "        last_upload_time = None\n",
    "\n",
    "    # 3. 뉴스 데이터의 기사 작성자\n",
    "    author = None\n",
    "\n",
    "    # 4. 뉴스 데이터의 본문\n",
    "    article = soup.find('div', id='article')\n",
    "    if article is None:\n",
    "        content = ''\n",
    "    else:\n",
    "        while article.find(\"td\") is not None:\n",
    "            article.find(\"td\").decompose()\n",
    "        news_content = article.get_text(separator=\"\\n\", strip=True).split('\\n')\n",
    "        # 뉴스 데이터 본문의 데이터 전처리1\n",
    "        # 디셉터에서 읽기 / Provided COINNESS / 이승훈 기자 123@gmail.com 등의 불필요한 텍스트 제거\n",
    "        while news_content:\n",
    "            if  p1.search(string=news_content[-1]) is None:\n",
    "                break\n",
    "            del news_content[-1]\n",
    "            \n",
    "        # 뉴스 데이터 본문의 데이터 전처리2\n",
    "        # - 하나의 텍스트로 결합한 다음, 맨 앞/뒤의 화이트스페이스(whitespace) 문자 제거\n",
    "        # - \"발표했습니다.그리고\"와 같이 마침표 다음에 유니코드 문자열이 바로 붙어있는 경우, 띄어쓰기 보정\n",
    "        # - \"\\xa0\"를 \" \"로 변경\n",
    "        content = '\\n\\n'.join(news_content)\n",
    "        content = p2.sub(repl=r'\\g<1>. \\g<2>', string=content)\n",
    "        content = p3.sub(repl=' ', string=content)\n",
    "        content = content.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 5. 뉴스 웹사이트 이름\n",
    "    website = 'Investing'\n",
    "\n",
    "    # 6. 비고\n",
    "    note = None\n",
    "\n",
    "    info['news_title'] = title\n",
    "    info['news_first_upload_time'] = first_upload_time\n",
    "    info['news_last_upload_time'] = last_upload_time\n",
    "    info['author'] = author\n",
    "    info['news_content'] = content\n",
    "    info['news_url'] = news_url\n",
    "    info['news_website'] = website\n",
    "    info['note'] = note\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1733669946843,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "3MFKvcUcUq9_"
   },
   "outputs": [],
   "source": [
    "def hankyung(\n",
    "            news_url: str, headers: dict[str, str], allow_redirects: bool, timeout: int, max_retry: int,\n",
    "              p0: re.Pattern\n",
    "              ) -> dict[str, str, None] | None:\n",
    "    \"\"\"뉴스 URL을 바탕으로 크롤링을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        news_url_tag: 뉴스 URL\n",
    "        headers: 식별 정보\n",
    "        allow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        p0: 인도 벵갈루루=양한나 블루밍비트 기자 sheep@bloomingbit.io / (사진=연합뉴스)와같은 텍스트 패턴\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"news_title\": 뉴스 제목, str\n",
    "            \"news_first_upload_time\": 뉴스 최초 업로드 시각, str | None\n",
    "            \"newsfinal_upload_time\": 뉴스 최종 수정 시각, str | None\n",
    "            \"author\": 뉴스 작성자, str | None\n",
    "            \"news_content\": 뉴스 본문, str\n",
    "            \"news_url\": 뉴스 URL, str\n",
    "            \"news_website\": 뉴스 웹사이트, str\n",
    "            \"note\": 비고, str | None\n",
    "        }\n",
    "\n",
    "        or\n",
    "\n",
    "        None\n",
    "    \"\"\"\n",
    "    info = {} # 뉴스 데이터 정보 Dictionary\n",
    "\n",
    "    # requests로 HTML GET\n",
    "    html = request_html(url=news_url, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "    # HTML 문서 정보를 불러오는 것에 실패하면 None 반환\n",
    "    if html is None:\n",
    "        return None\n",
    "    # BeautifulSoup로 parser\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 1. 뉴스 데이터의 제목\n",
    "    title = soup.find('h1', {\"class\": \"headline\"})\n",
    "    if title is None:\n",
    "        title = None\n",
    "    else:\n",
    "        title = title.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 2. 뉴스 데이터의 최초 업로드 시각과 최종 수정 시각\n",
    "    upload_times = soup.find_all('span', {\"class\": \"txt-date\"})\n",
    "    if upload_times:\n",
    "        first_upload_time = upload_times[0].text\n",
    "        first_upload_time = datetime.strptime(first_upload_time, '%Y.%m.%d %H:%M')\n",
    "        first_upload_time = datetime.strftime(first_upload_time, '%Y-%m-%d %p %I:%M')\n",
    "        last_upload_time = upload_times[1].text\n",
    "        last_upload_time = datetime.strptime(last_upload_time, '%Y.%m.%d %H:%M')\n",
    "        last_upload_time = datetime.strftime(last_upload_time, '%Y-%m-%d %p %I:%M')\n",
    "    else:\n",
    "        first_upload_time = None\n",
    "        last_upload_time = None\n",
    "\n",
    "    # 3. 뉴스 데이터의 기사 작성자\n",
    "    author_list = soup.find_all('div', {\"class\": \"author link subs_author_list\"})\n",
    "    if author_list:\n",
    "        author_list = map(lambda x: x.find(\"a\").text, author_list)\n",
    "        author = ', '.join(author_list)\n",
    "    else:\n",
    "        author = None\n",
    "\n",
    "    # 4. 뉴스 데이터의 본문\n",
    "    articletxt = soup.find(\"div\", id=\"articletxt\")\n",
    "    while articletxt.find(\"strong\") is not None:\n",
    "        articletxt.find(\"strong\").decompose()\n",
    "    while articletxt.find(\"figcaption\") is not None:\n",
    "        articletxt.find(\"figcaption\").decompose()\n",
    "    text_list = articletxt.get_text(separator=\"\\n\", strip=True).split('\\n')\n",
    "    while text_list:\n",
    "        if p0.search(string=text_list[-1]) is None:\n",
    "            break\n",
    "        del text_list[-1]\n",
    "        \n",
    "    content = '\\n\\n'.join(text_list).strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 5. 뉴스 웹사이트 이름\n",
    "    website = 'Hankyung'\n",
    "\n",
    "    # 6. 비고\n",
    "    note = None\n",
    "\n",
    "    info['news_title'] = title\n",
    "    info['news_first_upload_time'] = first_upload_time\n",
    "    info['news_last_upload_time'] = last_upload_time\n",
    "    info['author'] = author\n",
    "    info['news_content'] = content\n",
    "    info['news_url'] = news_url\n",
    "    info['news_website'] = website\n",
    "    info['note'] = note\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1733669946843,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "ham3rmXQ2IDA"
   },
   "outputs": [],
   "source": [
    "def crawling(website: str, news_websites_dict: dict[str, str]):\n",
    "    \"\"\"Crawling을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        website: 웹사이트 이름, str\n",
    "        news_websites_dict: 웹사이트 Dictionary, dict[str, str]\n",
    "\n",
    "    Returns:\n",
    "        pass\n",
    "    \"\"\"\n",
    "\n",
    "    assert website in news_websites_dict, f'{website} is not target website.'\n",
    "\n",
    "    match website:\n",
    "        case 'Investing':\n",
    "            pass\n",
    "\n",
    "        case 'Hankyung':\n",
    "            pass\n",
    "\n",
    "        case 'Bloomingbit':\n",
    "            pass\n",
    "\n",
    "        case 'Coinreaders':\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전역 변수 및 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# cpu 갯수\n",
    "workers = os.cpu_count()\n",
    "print(workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_websites_dict = {\n",
    "                        'Investing': 'https://kr.investing.com/news/cryptocurrency-news',\n",
    "                        'Hankyung': 'https://www.hankyung.com/koreamarket/news/crypto',\n",
    "                        'Bloomingbit': 'https://bloomingbit.io/feed',\n",
    "                        'Coinreaders': 'https://www.coinreaders.com/'\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Agent 변경을 위한 옵션 설정\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "headers = {'User-Agent': user_agent}\n",
    "\n",
    "# requests 파라미터\n",
    "allow_redirects = True # 리다이렉트 허용 여부\n",
    "timeout = 150 # 응답 대기 허용 시간\n",
    "max_retry = 10 # HTML 문서 요청 최대 재시도 횟수\n",
    "\n",
    "# 경로\n",
    "investing_path = r'.\\Data\\Investing_Data.json'\n",
    "hankyung_path = r'.\\Data\\Hankyung_Data.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bycij_yb2IDB"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwxuzzaF2IDC"
   },
   "source": [
    "## https://kr.investing.com/news/cryptocurrency-news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUegd-BSCjwG"
   },
   "source": [
    "### 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1263346,
     "status": "ok",
     "timestamp": 1733671553966,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "Ltv-G6ZTU_0p",
    "outputId": "1810bf2b-abe8-48f8-843d-dbf40bc19dc5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 697/2000 [1:13:05<2:15:08,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-698번 페이지-\n",
      "{'news_title': '[도기자의 한 주 정리] 비트코인 상승 랠리 놓쳤다면 주목해야 할 주식 12개', 'news_first_upload_time': '2021-10-15 PM 05:30', 'news_last_upload_time': None, 'author': None, 'news_content': '비트코인\\n(BTC) 상승 랠리가 지속되면서 암호화폐 시장에 대한 투자자 관심도 높아지고 있습니다. 그런데 만약 이번 기회를 놓쳤다면 어떻게 해야 할까요? 뱅크오브아메리카는 주식에서 기회를 노리라고 조언했습니다. 뱅크오브아메리카가 지목한 12개 종목에는 월트디즈니, 워너뮤직 등 큰 관련이 없어 보이는 종목도 있는데요.\\n자세한 내용 함께 살펴보도록 하겠습니다.\\n한 주 간 이슈를 콕 집어 정리해 드리는 도기자의 한 주 정리입니다.\\n?지난 12일(현지시간) CNBC는 뱅크오브아메리카 (NYSE:\\nBAC\\n) 애널리스트가 12개 종목을 추천했다고 보도했습니다. 뱅크오브아메리카는 “암호화폐는 무시하기에는 자산 규모가 너무 크다”며 “흐름을 타고 싶다면 주목해야 할 종목이 있다”고 분석했습니다.\\n━\\n암호화폐 사업에 적극적인 금융사 3곳\\n암호화폐 관련 금융주로는 페이팔(PYPL), JP모건체이스(JPM), 모건스탠리(MS)를 추천했습니다. 페이팔은 비트코인을 직접 매입하기도 했고, 관련 결제 시스템 도입에도 속도를 내고 있습니다. 모건스탠리도 지난 3월 일반 투자자 대상 비트코인 펀드 3종을 출시했습니다.\\n흥미로운 종목은 JP모건체이스입니다. 제이미 다이먼 JP모건체이스 회장은 암호화폐 비판론자로 알려져 있습니다. 비트코인에 자산가치가 없다고 비판했습니다. 수장이 나서서 비트코인을 비판하고 있지만 이는 개인 의견일 뿐 회사 차원에서 관련 사업에 발빠르게 진출했습니다. JP모건은 지난 2019년 미국계 은행으로는 최초로 암호화폐를 지원했습니다. 지난 4월에는 부유층 고객을 상대로 비트코인 펀드에 대한 투자 제안도 준비하고 있는 것으로 전해졌습니다. 앞뒤가 다른 모습은 또 다른 전략일까요? 속내가 궁금해집니다.\\n━\\nNFT 열기 뜨거워…관련 미디어 주\\n다음 분야는 암호화폐 관련 미디어주입니다. 폭스코퍼레이션(FOX), 아이하트미디어(IHRT), 월트디즈니(DIS), 워너뮤직(WMG)을 추천했습니다. 대체불가능한토큰(NFT)에 대한 투자 열기가 뜨거워지고 있어 이들 기업이 수혜를 볼 수 있다는 분석입니다.\\n━\\n채굴업자, 친환경에너지·저렴한 비용 선호…에너지 기업 수요 증가\\n이 외에도 암호화폐 관련 에너지주로 엑셀론(EXC), NRG에너지(NRG), 비스트라 에너지(VST)를 언급했습니다. 친환경 에너지, 저렴한 비용 등이 암호화폐 채굴에 있어 주요한 요소인 만큼 암호화폐 채굴업자 사이에서 수요가 증가할 수 있다는 평가입니다. 특히 원자력 에너지는 친화경적 요소가 24시간 가동되는 채굴기에 사용될 수 있다고 부연했습니다.\\n━\\n중국 규제로 채굴업자 미국으로 이동…데이터센터 관련주에 기회\\n마지막으로 데이터센터 관련주로 디지털리얼티(DLR), 이퀴닉스(EQIX)를 소개했습니다. 중국 규제로 암호화폐 채굴업자들이 북미지역으로 이동하고 있는데, 데이터 센터 기업들이 이 틈새시장을 공략해 기회를 노릴 수 있다는 진단입니다. 실제 13일(현지시간) 코인텔레그래프에 따르면 중국이 대대적으로 비트코인 채굴을 단속한 이후 미국이 해시레이트 점유율 1위로 올라섰죠.\\n뱅크오브아메리카는 이들 기업이 향후 12개월 안에 최소 10% 이상 오를 것이라고 전망했습니다. 이들 기업들이 향후 좋은 성적을 낼 수 있을지 기대가 됩니다.', 'news_url': 'https://kr.investing.com/news/cryptocurrency-news/article-708837', 'news_website': 'Investing', 'note': None}를 가져오는 것에 실패했습니다.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\project\\Lib\\site-packages\\requests\\models.py\", line 820, in generate\n",
      "    yield from self.raw.stream(chunk_size, decode_content=True)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\project\\Lib\\site-packages\\urllib3\\response.py\", line 1057, in stream\n",
      "    yield from self.read_chunked(amt, decode_content=decode_content)\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\project\\Lib\\site-packages\\urllib3\\response.py\", line 1206, in read_chunked\n",
      "    self._update_chunk_length()\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\project\\Lib\\site-packages\\urllib3\\response.py\", line 1136, in _update_chunk_length\n",
      "    raise ProtocolError(\"Response ended prematurely\") from None\n",
      "urllib3.exceptions.ProtocolError: Response ended prematurely\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6012\\1920617958.py\", line 31, in <module>\n",
      "    for idx, d in enumerate(data, start=1):\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\project\\Lib\\concurrent\\futures\\_base.py\", line 619, in result_iterator\n",
      "    yield _result_or_cancel(fs.pop())\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\project\\Lib\\concurrent\\futures\\_base.py\", line 317, in _result_or_cancel\n",
      "    return fut.result(timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\project\\Lib\\concurrent\\futures\\_base.py\", line 456, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\project\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\project\\Lib\\concurrent\\futures\\thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6012\\2758908460.py\", line 35, in investing\n",
      "    html = requests.get(url, headers=headers, allow_redirects=allow_redirects, timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\project\\Lib\\site-packages\\requests\\api.py\", line 73, in get\n",
      "    return request(\"get\", url, params=params, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\project\\Lib\\site-packages\\requests\\api.py\", line 59, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\project\\Lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\project\\Lib\\site-packages\\requests\\sessions.py\", line 746, in send\n",
      "    r.content\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\project\\Lib\\site-packages\\requests\\models.py\", line 902, in content\n",
      "    self._content = b\"\".join(self.iter_content(CONTENT_CHUNK_SIZE)) or b\"\"\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\anaconda3\\envs\\project\\Lib\\site-packages\\requests\\models.py\", line 822, in generate\n",
      "    raise ChunkedEncodingError(e)\n",
      "requests.exceptions.ChunkedEncodingError: Response ended prematurely\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [3:28:17<00:00,  6.25s/it]  \n"
     ]
    }
   ],
   "source": [
    "web_page = 'https://kr.investing.com/news/cryptocurrency-news'\n",
    "start = 1\n",
    "get_page_cnt = 2000\n",
    "end = start + get_page_cnt\n",
    "p0 = re.compile(r'\\s+\\r\\n\\s+')\n",
    "p1 = re.compile(r'(읽기|provided|[a-z0-9]@[a-z0-9]|무단전재|재배포|쿠폰코드)', flags=re.IGNORECASE)\n",
    "p2 = re.compile(r'([가-힣])\\.(\\w)')\n",
    "p3 = re.compile(r'(\\xa0)+')\n",
    "# 일부 파라미터들을 고정한 investing 함수 생성\n",
    "fixed_investing = partial(investing,\n",
    "                            headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry,\n",
    "                            p0=p0, p1=p1, p2=p2, p3=p3)\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(start, end)):\n",
    "    try:\n",
    "        html = requests.get(web_page, headers=headers, allow_redirects=allow_redirects, timeout=timeout).text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        url_tag_list = soup.find_all('article', {\"data-test\": \"article-item\"})\n",
    "        url_list = [url[\"href\"] for url_tag in url_tag_list if (url := url_tag.find('a') is not None)]\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'{i}번 페이지의 HTML 문서 정보를 불러오는데 실패했습니다.')\n",
    "        print(traceback.format_exc())\n",
    "        web_page = f'https://kr.investing.com/news/cryptocurrency-news/{i + 1}'\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        executor = futures.ThreadPoolExecutor(max_workers=workers)\n",
    "        data = executor.map(fixed_investing, url_list)\n",
    "\n",
    "        for idx, d in enumerate(data, start=1):\n",
    "            if d is not None:\n",
    "                results.append(d)\n",
    "            else:\n",
    "                print()\n",
    "                print(f'-{i}번 페이지-')\n",
    "                print(f'{i}번 페이지의 {idx}번째 데이터를 가져오는 것에 실패했습니다.')\n",
    "                print(f'실패한 뉴스 데이터의 URL : {url_tag_list[idx - 1].find(\"a\")[\"href\"]}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'-{i}번 페이지-')\n",
    "        print(f'{d}를 가져오는 것에 실패했습니다.')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    time.sleep(random.uniform(0.35, 1.25))\n",
    "    web_page = f'https://kr.investing.com/news/cryptocurrency-news/{i + 1}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVIrAc7ZCmQd"
   },
   "source": [
    "### JSON으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1733671674719,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "j1Hs6CWGcRYa"
   },
   "outputs": [],
   "source": [
    "with open(investing_path, 'w') as f:\n",
    "    json.dump(results, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxW9SY3VjBWk"
   },
   "source": [
    "## https://www.hankyung.com/koreamarket/news/crypto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BX74tfLYnZ3S"
   },
   "source": [
    "### 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11688,
     "status": "ok",
     "timestamp": 1733670082440,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "nvQK7B2Fi06x",
    "outputId": "ae2f0f40-ca1b-4b38-ada7-8d69b3c577b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [11:42<00:00,  3.51s/it]\n"
     ]
    }
   ],
   "source": [
    "start = 1\n",
    "get_page_cnt = 200\n",
    "end = start + get_page_cnt\n",
    "p0 = re.compile(r'([a-z0-9]@[a-z0-9]|사진=|\\b기자\\b|영상촬영\\s*:|한국경제TV [가-힣]{2,4}입니다)', flags=re.IGNORECASE)\n",
    "\n",
    "# 일부 파라미터들을 고정한 hankyung 함수 생성\n",
    "fixed_hankyung = partial(hankyung,\n",
    "                            headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry,\n",
    "                            p0=p0)\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(start, end)):\n",
    "    web_page = f'https://www.hankyung.com/koreamarket/news/crypto?page={i}'\n",
    "    try:\n",
    "        html = requests.get(web_page, headers=headers, allow_redirects=allow_redirects, timeout=timeout).text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        url_tag_list = soup.find_all('h2', {\"class\": \"news-tit\"})\n",
    "        url_list = [url[\"href\"] for url_tag in url_tag_list if (url := url_tag.find('a') is not None)]\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'{i}번 페이지의 HTML 문서 정보를 불러오는데 실패했습니다.')\n",
    "        print(traceback.format_exc())\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        executor = futures.ThreadPoolExecutor(max_workers=workers)\n",
    "        data = executor.map(fixed_hankyung, url_list)\n",
    "\n",
    "        for idx, d in enumerate(data, start=1):\n",
    "            if d is not None:\n",
    "                results.append(d)\n",
    "            else:\n",
    "                print()\n",
    "                print(f'-{i}번 페이지-')\n",
    "                print(f'{i}번 페이지의 {idx}번째 데이터를 가져오는 것에 실패했습니다.')\n",
    "                print(f'실패한 뉴스 데이터의 URL : {url_tag_list[idx - 1].find(\"a\")[\"href\"]}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'-{i}번 페이지-')\n",
    "        print(f'{d}를 가져오는 것에 실패했습니다.')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    time.sleep(random.uniform(0.35, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZb1FuoSZ0RJ"
   },
   "source": [
    "### JSON으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1733667830226,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "tF3M18pXZ0RK"
   },
   "outputs": [],
   "source": [
    "with open(hankyung_path, 'w') as f:\n",
    "    json.dump(results, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://kr.investing.com/news/cryptocurrency-news/article-1302958'\n",
    "html = requests.get(url, headers=headers, allow_redirects=allow_redirects, timeout=timeout).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = soup.find('div', id='article')\n",
    "content = article.find_all(re.compile(r'^(p|)$'))\n",
    "content = article.find_all(lambda tag: tag.name in ('p', 'span', 'h2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'가상자산법의 이해를 돕기 위해 영상 콘텐츠를 제작한 코빗은 오는 13일부터 2주 동안 소셜네트워크(SNS) 퀴즈 이벤트를 진행한다. 코빗 공식 유튜브 채널에서 ‘가상자산 이용자라면 알아야 할 Q&A’를 시청하고 퀴즈를 맞힌 이용자에게 추첨을 통해 상품을 제공한다.\\n\\n코빗은 영상을 통해 이용자 자산 보호와 거래 시 주의사항을 알리는 것에 중점을 뒀다. 또 이번 이벤트를 통해 이용자가 안전하게 가상자산 시장에 접근하도록 지원할 방침이다. 이벤트에 대한 자세한 내용은 코빗 공식 홈페이지와 애플리케이션(앱)에서 확인할 수 있다. 오세진 코빗 대표는 “가상자산법을 준수하고 내부통제를 강화해 바람직한 가상자산 거래 문화를 조성하겠다”고 말했다.\\n\\n가상자산법은 이용자 자산 보호를 강화하고 불공정 거래를 규제하기 위해 마련된 법안으로, 주요 내용은 △이용자 예치금 보호 △콜드월렛 보관 의무 △불공정 거래 감시 등이다.\\n\\n디센터에서 읽기'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\n\\n'.join(t.text for t in content).strip(' \\t\\n\\r\\f\\v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = article.select('#div > span')\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
