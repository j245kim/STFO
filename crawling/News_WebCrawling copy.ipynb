{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10814,
     "status": "ok",
     "timestamp": 1733586579007,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "oKtWa7v02ICx",
    "outputId": "010a47b4-bff7-4ac7-e294-59fc0f062de2"
   },
   "outputs": [],
   "source": [
    "# !pip install httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6792,
     "status": "ok",
     "timestamp": 1733586585797,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "2Zf1EfhFiiH9",
    "outputId": "6c49166f-fa78-4ce8-be90-231ebe539291"
   },
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4H1DYrC82ICy"
   },
   "source": [
    "# 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1733665110553,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "auvaotrw2ICz"
   },
   "outputs": [],
   "source": [
    "# 파이썬 표준 라이브러리\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import asyncio\n",
    "import logging\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from concurrent import futures\n",
    "\n",
    "# 파이썬 서드파티 라이브러리\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxDIDmyG2IDA"
   },
   "source": [
    "# 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_request(\n",
    "                url: str, headers: dict[str, str], follow_redirects: bool = True, timeout: int | float = 90,\n",
    "                encoding: str = 'utf-8', max_retry: int = 10, min_delay: int | float = 0.55, max_delay: int | float = 1.55\n",
    "                 ) -> dict[str, str, None]:\n",
    "    \"\"\"동기로 HTML 문서 정보를 불러오는 함수\n",
    "\n",
    "    Args:\n",
    "        url: URL\n",
    "        headers: 식별 정보\n",
    "        follow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        encoding: 인코딩 방법\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        min_delay: 재시도 할 때 딜레이의 최소 시간\n",
    "        max_delay: 재시도 할 때 딜레이의 최대 시간\n",
    "    \n",
    "    Return:\n",
    "        {\n",
    "            \"html\": HTML 문서 정보, str | None\n",
    "            \"response_reason\": 응답 결과 이유, str | None\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    result = {\"html\": None, \"response_reason\": None}\n",
    "    client = httpx.Client(headers=headers, follow_redirects=follow_redirects, timeout=timeout, default_encoding=encoding)\n",
    "\n",
    "    for _ in range(max_retry):\n",
    "        # 동기 client로 HTML GET\n",
    "        response = client.get(url)\n",
    "        # HTML 문서 정보를 불러오는 것에 성공하면 for문 중단\n",
    "        if response.status_code == httpx.codes.ok:\n",
    "            result['html'] = response.text\n",
    "            break\n",
    "\n",
    "        # 동기 제어 유지(멀티 프로세싱이라는 전제)\n",
    "        time.sleep(random.uniform(min_delay, max_delay))\n",
    "    \n",
    "    # 응답 요청이 실패했으면 메세지 출력\n",
    "    if result['html'] is None:\n",
    "        result['response_reason'] = response.reason_phrase\n",
    "    \n",
    "    client.close()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_request(\n",
    "                url: str, headers: dict[str, str], follow_redirects: bool = True, timeout: int | float = 90,\n",
    "                encoding: str = 'utf-8', max_retry: int = 10, min_delay: int | float = 0.55, max_delay: int | float = 1.55\n",
    "                 ) -> dict[str, str, None]:\n",
    "    \"\"\"비동기로 HTML 문서 정보를 불러오는 함수\n",
    "\n",
    "    Args:\n",
    "        url: URL\n",
    "        headers: 식별 정보\n",
    "        follow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        encoding: 인코딩 방법\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        min_delay: 재시도 할 때 딜레이의 최소 시간\n",
    "        max_delay: 재시도 할 때 딜레이의 최대 시간\n",
    "    \n",
    "    Return:\n",
    "        {\n",
    "            \"html\": HTML 문서 정보, str | None\n",
    "            \"response_reason\": 응답 결과 이유, str | None\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    result = {\"html\": None, \"response_reason\": None}\n",
    "\n",
    "    for _ in range(max_retry):\n",
    "        # 비동기 client로 HTML GET\n",
    "        async with httpx.AsyncClient(headers=headers, follow_redirects=follow_redirects, timeout=timeout, default_encoding=encoding) as client:\n",
    "            response = await client.get(url)\n",
    "        # HTML 문서 정보를 불러오는 것에 성공하면 for문 중단\n",
    "        if response.status_code == httpx.codes.ok:\n",
    "            result['html'] = response.text\n",
    "            break\n",
    "\n",
    "        # 비동기 코루틴 제어 양도\n",
    "        await asyncio.sleep(random.uniform(min_delay, max_delay))\n",
    "    \n",
    "    # 응답 요청이 실패했으면 메세지 출력\n",
    "    if result['html'] is None:\n",
    "        result['response_reason'] = response.reason_phrase\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def news_crawling(\n",
    "                        url:str, category: str, website: str,\n",
    "                        headers: dict[str, str], follow_redirects: bool = True, timeout: int | float = 90,\n",
    "                        encoding: str = 'utf-8', max_retry: int = 10, min_delay: int | float = 0.55, max_delay: int | float = 1.55\n",
    "                        ) -> dict[str, str, None] | None:\n",
    "    \"\"\"뉴스 URL을 바탕으로 크롤링을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        url: 뉴스 URL\n",
    "        category: 뉴스 카테고리\n",
    "        website: 웹사이트 이름\n",
    "        headers: 식별 정보\n",
    "        follow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        encoding: 인코딩 방법\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        min_delay: 재시도 할 때 딜레이의 최소 시간\n",
    "        max_delay: 재시도 할 때 딜레이의 최대 시간\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"news_title\": 뉴스 제목, str\n",
    "            \"news_first_upload_time\": 뉴스 최초 업로드 시각, str | None\n",
    "            \"newsfinal_upload_time\": 뉴스 최종 수정 시각, str | None\n",
    "            \"news_author\": 뉴스 작성자, str | None\n",
    "            \"news_content\": 뉴스 본문, str\n",
    "            \"news_url\": 뉴스 URL, str\n",
    "            \"news_category\": 뉴스 카테고리, str\n",
    "            \"news_website\": 뉴스 웹사이트, str\n",
    "            \"note\": 비고, str | None\n",
    "        }\n",
    "\n",
    "        or\n",
    "\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    info = {} # 뉴스 데이터 정보 Dictionary\n",
    "\n",
    "    # 비동기로 HTML GET\n",
    "    result = await async_request(url=url, headers=headers, follow_redirects=follow_redirects, timeout=timeout,\n",
    "                               encoding=encoding, max_retry=max_retry, min_delay=min_delay, max_delay=max_delay)\n",
    "    # HTML 문서 정보를 불러오는 것에 실패하면 None 반환\n",
    "    if result['html'] is None:\n",
    "        return None\n",
    "    # BeautifulSoup로 parser\n",
    "    soup = BeautifulSoup(result['html'], 'html.parser')\n",
    "\n",
    "    match website:\n",
    "        case 'inveseting':\n",
    "            # 1. 뉴스 데이터의 제목\n",
    "            title = soup.find('h1', id='articleTitle')\n",
    "            title = title.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "            # 2. 뉴스 데이터의 최초 업로드 시각과 최종 수정 시각\n",
    "            div = soup.find_all('div', {'class': 'flex flex-row items-center'})\n",
    "            span = div[1].find('span')\n",
    "            span = span.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "            first_upload_time_list = re.split(pattern=r'\\s+\\r\\n\\s+', string=span)[1].split()\n",
    "            y_m_d = '-'.join(times[:-1] for times in first_upload_time_list[:3])\n",
    "            if first_upload_time_list[3] == '오전':\n",
    "                ap = 'AM'\n",
    "            else:\n",
    "                ap = 'PM'\n",
    "\n",
    "            first_upload_time = y_m_d + ' ' + ap + ' ' + first_upload_time_list[4]\n",
    "            last_upload_time = None\n",
    "            \n",
    "            # 3. 뉴스 데이터의 기사 작성자\n",
    "            author = None\n",
    "\n",
    "            # 4. 뉴스 데이터의 본문\n",
    "            content = soup.find('div', id='article')\n",
    "        case 'cryptonews':\n",
    "            pass\n",
    "        case 'hankyung':\n",
    "            # 1. 뉴스 데이터의 제목\n",
    "            title = soup.find('h1', {\"class\": \"headline\"})\n",
    "            title = title.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "            # 2. 뉴스 데이터의 최초 업로드 시각과 최종 수정 시각\n",
    "            upload_times = soup.find_all('span', {\"class\": \"txt-date\"})\n",
    "\n",
    "            first_upload_time = upload_times[0].text\n",
    "            first_upload_time = datetime.strptime(first_upload_time, '%Y.%m.%d %H:%M')\n",
    "            first_upload_time = datetime.strftime(first_upload_time, '%Y-%m-%d %p %I:%M')\n",
    "            last_upload_time = upload_times[1].text\n",
    "            last_upload_time = datetime.strptime(last_upload_time, '%Y.%m.%d %H:%M')\n",
    "            last_upload_time = datetime.strftime(last_upload_time, '%Y-%m-%d %p %I:%M')\n",
    "\n",
    "            # 3. 뉴스 데이터의 기사 작성자\n",
    "            author_list = soup.find_all('div', {\"class\": \"author link subs_author_list\"})\n",
    "            author_list = map(lambda x: x.find(\"a\").text, author_list)\n",
    "            author = ', '.join(author_list)\n",
    "\n",
    "            # 4. 뉴스 데이터의 본문\n",
    "            content = soup.find(\"div\", id=\"articletxt\")\n",
    "        case 'bloomingbit':\n",
    "            pass\n",
    "        case 'coinreaders':\n",
    "            pass\n",
    "        case 'dealsite':\n",
    "            pass\n",
    "        case 'blockstreet':\n",
    "            pass\n",
    "    \n",
    "\n",
    "    # 7. 비고\n",
    "    note = None\n",
    "\n",
    "    info['news_title'] = title\n",
    "    info['news_first_upload_time'] = first_upload_time\n",
    "    info['news_last_upload_time'] = last_upload_time\n",
    "    info['news_author'] = author\n",
    "    info['news_content'] = content\n",
    "    info['news_url'] = url\n",
    "    info['news_category'] = category\n",
    "    info['news_website'] = website\n",
    "    info['note'] = note\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def async_main(\n",
    "                url_list: list[str], category: str, website: str,\n",
    "                headers: dict[str, str], follow_redirects: bool = True, timeout: int | float = 90,\n",
    "                encoding: str = 'utf-8', max_retry: int = 10, min_delay: int | float = 0.55, max_delay: int | float = 1.55\n",
    "                ) -> list[dict[str, str, None]]:\n",
    "    \n",
    "    crawl_list = [news_crawling(url=url, category=category, website=website, headers=headers) for url in url_list]\n",
    "    result = asyncio.gather(*crawl_list)\n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investing(\n",
    "                headers: dict[str, str], follow_redirects: bool = True, timeout: int | float = 90,\n",
    "                encoding: str = 'utf-8', max_retry: int = 10, min_delay: int | float = 0.55, max_delay: int | float = 1.55\n",
    "                ) -> list[dict[str, str, None]]:\n",
    "    \"\"\"investing 사이트를 크롤링 하는 함수\n",
    "\n",
    "    Args:\n",
    "        headers: 식별 정보\n",
    "        follow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        encoding: 인코딩 방법\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        min_delay: 재시도 할 때 딜레이의 최소 시간\n",
    "        max_delay: 재시도 할 때 딜레이의 최대 시간\n",
    "\n",
    "    Returns:\n",
    "        [\n",
    "            {\n",
    "                \"news_title\": 뉴스 제목, str\n",
    "                \"news_first_upload_time\": 뉴스 최초 업로드 시각, str | None\n",
    "                \"newsfinal_upload_time\": 뉴스 최종 수정 시각, str | None\n",
    "                \"news_author\": 뉴스 작성자, str | None\n",
    "                \"news_content\": 뉴스 본문, str\n",
    "                \"news_url\": 뉴스 URL, str\n",
    "                \"news_category\": 뉴스 카테고리, str\n",
    "                \"news_website\": 뉴스 웹사이트, str\n",
    "                \"note\": 비고, str | None\n",
    "            },\n",
    "            {\n",
    "                                    ...\n",
    "            },\n",
    "                                    .\n",
    "                                    .\n",
    "                                    .\n",
    "        ]\n",
    "    \"\"\"\n",
    "\n",
    "    web_page = 'https://kr.investing.com/news/cryptocurrency-news'\n",
    "    start = 1\n",
    "    get_page_cnt = 30\n",
    "    end = start + get_page_cnt\n",
    "\n",
    "    investing_results = []\n",
    "\n",
    "    for i in tqdm(range(start, end), mininterval=1, miniters=1):\n",
    "        result = sync_request(url=web_page, headers=headers)\n",
    "\n",
    "        if result['html'] is None:\n",
    "            print()\n",
    "            print(f'{i}번 페이지의 HTML 문서 정보를 불러오는데 실패했습니다.')\n",
    "            print(traceback.format_exc())\n",
    "            web_page = f'https://kr.investing.com/news/cryptocurrency-news/{i + 1}'\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(result['html'], 'html.parser')\n",
    "        url_tag_list = soup.find_all('article', {\"data-test\": \"article-item\"})\n",
    "        url_list = [url[\"href\"] for url_tag in url_tag_list if ((url := url_tag.find('a')) is not None)]\n",
    "        result = asyncio.run(async_main(url_list, category='암호화폐', website='investing', headers=headers))\n",
    "        investing_results.extend(result)\n",
    "\n",
    "        time.sleep(random.uniform(min_delay, max_delay))\n",
    "        web_page = f'https://kr.investing.com/news/cryptocurrency-news/{i + 1}'\n",
    "    \n",
    "    return investing_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def hankyung():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전역 변수 및 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# cpu 갯수\n",
    "workers = os.cpu_count()\n",
    "print(workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Agent 변경을 위한 옵션 설정\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "headers = {'User-Agent': user_agent}\n",
    "\n",
    "# client 파라미터\n",
    "follow_redirects = True # 리다이렉트 허용 여부\n",
    "timeout = 90 # 응답 대기 허용 시간\n",
    "encoding = 'utf-8'\n",
    "max_retry = 10 # HTML 문서 요청 최대 재시도 횟수\n",
    "min_delay = 0.55 # 재시도 할 때 딜레이의 최소 시간\n",
    "max_delay = 1.55 # 재시도 할 때 딜레이의 최대 시간\n",
    "\n",
    "# 경로\n",
    "investing_path = r'.\\datas\\news_data\\Investing_Data.json'\n",
    "hankyung_path = r'.\\datas\\news_data\\Hankyung_Data.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bycij_yb2IDB"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://kr.investing.com/news/cryptocurrency-news'\n",
    "\n",
    "# User-Agent 변경을 위한 옵션 설정\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "headers = {'User-Agent': user_agent}\n",
    "\n",
    "# client 파라미터\n",
    "follow_redirects = True # 리다이렉트 허용 여부\n",
    "timeout = 90 # 응답 대기 허용 시간\n",
    "encoding = 'utf-8'\n",
    "max_retry = 10 # HTML 문서 요청 최대 재시도 횟수\n",
    "min_delay = 0.55 # 재시도 할 때 딜레이의 최소 시간\n",
    "max_delay = 1.55 # 재시도 할 때 딜레이의 최대 시간\n",
    "\n",
    "# 경로\n",
    "investing_path = r'.\\datas\\news_data\\Investing_Data.json'\n",
    "hankyung_path = r'.\\datas\\news_data\\Hankyung_Data.json'\n",
    "\n",
    "response = sync_request(url=url, headers=headers, follow_redirects=follow_redirects, timeout=timeout, encoding=encoding,\n",
    "                        max_retry=max_retry, min_delay=min_delay, max_delay=max_delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[169], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43minvesting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[165], line 58\u001b[0m, in \u001b[0;36minvesting\u001b[1;34m(headers, follow_redirects, timeout, encoding, max_retry, min_delay, max_delay)\u001b[0m\n\u001b[0;32m     56\u001b[0m url_tag_list \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata-test\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle-item\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m     57\u001b[0m url_list \u001b[38;5;241m=\u001b[39m [url[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m url_tag \u001b[38;5;129;01min\u001b[39;00m url_tag_list \u001b[38;5;28;01mif\u001b[39;00m ((url \u001b[38;5;241m:=\u001b[39m url_tag\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)]\n\u001b[1;32m---> 58\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43masync_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m암호화폐\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwebsite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minvesting\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m investing_results\u001b[38;5;241m.\u001b[39mextend(result)\n\u001b[0;32m     61\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39muniform(min_delay, max_delay))\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\project\\Lib\\asyncio\\runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=UnboundLocalError(\"cannot access local variable 'title' where it is not associated with a value\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2116\\2558777405.py\", line 110, in news_crawling\n",
      "    note = None\n",
      "    ^^^^\n",
      "UnboundLocalError: cannot access local variable 'title' where it is not associated with a value\n"
     ]
    }
   ],
   "source": [
    "result = investing(headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
