{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10814,
     "status": "ok",
     "timestamp": 1733586579007,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "oKtWa7v02ICx",
    "outputId": "010a47b4-bff7-4ac7-e294-59fc0f062de2"
   },
   "outputs": [],
   "source": [
    "# !pip install httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6792,
     "status": "ok",
     "timestamp": 1733586585797,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "2Zf1EfhFiiH9",
    "outputId": "6c49166f-fa78-4ce8-be90-231ebe539291"
   },
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4H1DYrC82ICy"
   },
   "source": [
    "# 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1733665110553,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "auvaotrw2ICz"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'project (Python 3.11.11)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 파이썬 표준 라이브러리\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import asyncio\n",
    "import logging\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from concurrent import futures\n",
    "\n",
    "# 파이썬 서드파티 라이브러리\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxDIDmyG2IDA"
   },
   "source": [
    "# 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_request(\n",
    "                url: str, headers: dict[str, str], follow_redirects: bool = True, timeout: int | float = 90,\n",
    "                encoding: str = 'utf-8', max_retry: int = 10, min_delay: int | float = 0.55, max_delay: int | float = 1.55\n",
    "                 ) -> dict[str, str, None]:\n",
    "    \"\"\"동기로 HTML 문서 정보를 불러오는 함수\n",
    "\n",
    "    Args:\n",
    "        url: URL\n",
    "        headers: 식별 정보\n",
    "        follow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        encoding: 인코딩 방법\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        min_delay: 재시도 할 때 딜레이의 최소 시간\n",
    "        max_delay: 재시도 할 때 딜레이의 최대 시간\n",
    "    \n",
    "    Return:\n",
    "        {\n",
    "            \"html\": HTML 문서 정보, str | None\n",
    "            \"response_reason\": 응답 결과 이유, str | None\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    result = {\"html\": None, \"response_reason\": None}\n",
    "    client = httpx.Client(headers=headers, follow_redirects=follow_redirects, timeout=timeout, default_encoding=encoding)\n",
    "\n",
    "    for _ in range(max_retry):\n",
    "        # 동기 client로 HTML GET\n",
    "        response = client.get(url)\n",
    "        # HTML 문서 정보를 불러오는 것에 성공하면 for문 중단\n",
    "        if response.status_code == httpx.codes.ok:\n",
    "            result['html'] = response.text\n",
    "            break\n",
    "\n",
    "        # 동기 제어 유지(멀티 프로세싱이라는 전제)\n",
    "        time.sleep(random.uniform(min_delay, max_delay))\n",
    "    \n",
    "    # 응답 요청이 실패했으면 메세지 출력\n",
    "    if result['html'] is None:\n",
    "        result['response_reason'] = response.reason_phrase\n",
    "    \n",
    "    client.close()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_request(\n",
    "                url: str, headers: dict[str, str], follow_redirects: bool = True, timeout: int | float = 90,\n",
    "                encoding: str = 'utf-8', max_retry: int = 10, min_delay: int | float = 0.55, max_delay: int | float = 1.55\n",
    "                 ) -> dict[str, str, None]:\n",
    "    \"\"\"비동기로 HTML 문서 정보를 불러오는 함수\n",
    "\n",
    "    Args:\n",
    "        url: URL\n",
    "        headers: 식별 정보\n",
    "        follow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        encoding: 인코딩 방법\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        min_delay: 재시도 할 때 딜레이의 최소 시간\n",
    "        max_delay: 재시도 할 때 딜레이의 최대 시간\n",
    "    \n",
    "    Return:\n",
    "        {\n",
    "            \"html\": HTML 문서 정보, str | None\n",
    "            \"response_reason\": 응답 결과 이유, str | None\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    result = {\"html\": None, \"response_reason\": None}\n",
    "\n",
    "    for _ in range(max_retry):\n",
    "        # 비동기 client로 HTML GET\n",
    "        async with httpx.AsyncClient(headers=headers, follow_redirects=follow_redirects, timeout=timeout, default_encoding=encoding) as client:\n",
    "            response = await client.get(url)\n",
    "        # HTML 문서 정보를 불러오는 것에 성공하면 for문 중단\n",
    "        if response.status_code == httpx.codes.ok:\n",
    "            result['html'] = response.text\n",
    "            break\n",
    "\n",
    "        # 비동기 코루틴 제어 양도\n",
    "        await asyncio.sleep(random.uniform(min_delay, max_delay))\n",
    "    \n",
    "    # 응답 요청이 실패했으면 메세지 출력\n",
    "    if result['html'] is None:\n",
    "        result['response_reason'] = response.reason_phrase\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def news_crawling(\n",
    "                        url:str, category: str, website: str,\n",
    "                        headers: dict[str, str], follow_redirects: bool = True, timeout: int | float = 90,\n",
    "                        encoding: str = 'utf-8', max_retry: int = 10, min_delay: int | float = 0.55, max_delay: int | float = 1.55\n",
    "                        ) -> dict[str, str, None] | None:\n",
    "    \"\"\"뉴스 URL을 바탕으로 크롤링을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        url: 뉴스 URL\n",
    "        category: 뉴스 카테고리\n",
    "        website: 웹사이트 이름\n",
    "        headers: 식별 정보\n",
    "        follow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        encoding: 인코딩 방법\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        min_delay: 재시도 할 때 딜레이의 최소 시간\n",
    "        max_delay: 재시도 할 때 딜레이의 최대 시간\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"news_title\": 뉴스 제목, str\n",
    "            \"news_first_upload_time\": 뉴스 최초 업로드 시각, str | None\n",
    "            \"newsfinal_upload_time\": 뉴스 최종 수정 시각, str | None\n",
    "            \"news_author\": 뉴스 작성자, str | None\n",
    "            \"news_content\": 뉴스 본문, str\n",
    "            \"news_url\": 뉴스 URL, str\n",
    "            \"news_category\": 뉴스 카테고리, str\n",
    "            \"news_website\": 뉴스 웹사이트, str\n",
    "            \"note\": 비고, str | None\n",
    "        }\n",
    "\n",
    "        or\n",
    "\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    info = {} # 뉴스 데이터 정보 Dictionary\n",
    "\n",
    "    # 비동기로 HTML GET\n",
    "    result = await async_request(url=url, headers=headers, follow_redirects=follow_redirects, timeout=timeout,\n",
    "                               encoding=encoding, max_retry=max_retry, min_delay=min_delay, max_delay=max_delay)\n",
    "    # HTML 문서 정보를 불러오는 것에 실패하면 None 반환\n",
    "    if result['html'] is None:\n",
    "        return None\n",
    "    # BeautifulSoup로 parser\n",
    "    soup = BeautifulSoup(result['html'], 'html.parser')\n",
    "\n",
    "    match website:\n",
    "        case 'inveseting':\n",
    "            # 1. 뉴스 데이터의 제목\n",
    "            title = soup.find('h1', id='articleTitle')\n",
    "            title = title.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "            # 2. 뉴스 데이터의 최초 업로드 시각과 최종 수정 시각\n",
    "            div = soup.find_all('div', {'class': 'flex flex-row items-center'})\n",
    "            span = div[1].find('span')\n",
    "            span = span.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "            first_upload_time_list = re.split(pattern=r'\\s+\\r\\n\\s+', string=span)[1].split()\n",
    "            y_m_d = '-'.join(times[:-1] for times in first_upload_time_list[:3])\n",
    "            if first_upload_time_list[3] == '오전':\n",
    "                ap = 'AM'\n",
    "            else:\n",
    "                ap = 'PM'\n",
    "\n",
    "            first_upload_time = y_m_d + ' ' + ap + ' ' + first_upload_time_list[4]\n",
    "            last_upload_time = None\n",
    "            \n",
    "            # 3. 뉴스 데이터의 기사 작성자\n",
    "            author = None\n",
    "\n",
    "            # 4. 뉴스 데이터의 본문\n",
    "            content = soup.find('div', id='article')\n",
    "        case 'cryptonews':\n",
    "            pass\n",
    "        case 'hankyung':\n",
    "            # 1. 뉴스 데이터의 제목\n",
    "            title = soup.find('h1', {\"class\": \"headline\"})\n",
    "            title = title.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "            # 2. 뉴스 데이터의 최초 업로드 시각과 최종 수정 시각\n",
    "            upload_times = soup.find_all('span', {\"class\": \"txt-date\"})\n",
    "\n",
    "            first_upload_time = upload_times[0].text\n",
    "            first_upload_time = datetime.strptime(first_upload_time, '%Y.%m.%d %H:%M')\n",
    "            first_upload_time = datetime.strftime(first_upload_time, '%Y-%m-%d %p %I:%M')\n",
    "            last_upload_time = upload_times[1].text\n",
    "            last_upload_time = datetime.strptime(last_upload_time, '%Y.%m.%d %H:%M')\n",
    "            last_upload_time = datetime.strftime(last_upload_time, '%Y-%m-%d %p %I:%M')\n",
    "\n",
    "            # 3. 뉴스 데이터의 기사 작성자\n",
    "            author_list = soup.find_all('div', {\"class\": \"author link subs_author_list\"})\n",
    "            author_list = map(lambda x: x.find(\"a\").text, author_list)\n",
    "            author = ', '.join(author_list)\n",
    "\n",
    "            # 4. 뉴스 데이터의 본문\n",
    "            content = soup.find(\"div\", id=\"articletxt\")\n",
    "        case 'bloomingbit':\n",
    "            pass\n",
    "        case 'coinreaders':\n",
    "            pass\n",
    "        case 'dealsite':\n",
    "            pass\n",
    "        case 'blockstreet':\n",
    "            pass\n",
    "    \n",
    "\n",
    "    # 7. 비고\n",
    "    note = None\n",
    "\n",
    "    info['news_title'] = title\n",
    "    info['news_first_upload_time'] = first_upload_time\n",
    "    info['news_last_upload_time'] = last_upload_time\n",
    "    info['news_author'] = author\n",
    "    info['news_content'] = content\n",
    "    info['news_url'] = url\n",
    "    info['news_category'] = category\n",
    "    info['news_website'] = website\n",
    "    info['note'] = note\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def async_main(\n",
    "                url_list: list[str], category: str, website: str,\n",
    "                headers: dict[str, str], follow_redirects: bool = True, timeout: int | float = 90,\n",
    "                encoding: str = 'utf-8', max_retry: int = 10, min_delay: int | float = 0.55, max_delay: int | float = 1.55\n",
    "                ) -> list[dict[str, str, None]]:\n",
    "    \n",
    "    crawl_list = [news_crawling(url=url, category=category, website=website, headers=headers) for url in url_list]\n",
    "    result = asyncio.gather(*crawl_list)\n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investing(\n",
    "                headers: dict[str, str], follow_redirects: bool = True, timeout: int | float = 90,\n",
    "                encoding: str = 'utf-8', max_retry: int = 10, min_delay: int | float = 0.55, max_delay: int | float = 1.55\n",
    "                ) -> list[dict[str, str, None]]:\n",
    "    \"\"\"investing 사이트를 크롤링 하는 함수\n",
    "\n",
    "    Args:\n",
    "        headers: 식별 정보\n",
    "        follow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        encoding: 인코딩 방법\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        min_delay: 재시도 할 때 딜레이의 최소 시간\n",
    "        max_delay: 재시도 할 때 딜레이의 최대 시간\n",
    "\n",
    "    Returns:\n",
    "        [\n",
    "            {\n",
    "                \"news_title\": 뉴스 제목, str\n",
    "                \"news_first_upload_time\": 뉴스 최초 업로드 시각, str | None\n",
    "                \"newsfinal_upload_time\": 뉴스 최종 수정 시각, str | None\n",
    "                \"news_author\": 뉴스 작성자, str | None\n",
    "                \"news_content\": 뉴스 본문, str\n",
    "                \"news_url\": 뉴스 URL, str\n",
    "                \"news_category\": 뉴스 카테고리, str\n",
    "                \"news_website\": 뉴스 웹사이트, str\n",
    "                \"note\": 비고, str | None\n",
    "            },\n",
    "            {\n",
    "                                    ...\n",
    "            },\n",
    "                                    .\n",
    "                                    .\n",
    "                                    .\n",
    "        ]\n",
    "    \"\"\"\n",
    "\n",
    "    web_page = 'https://kr.investing.com/news/cryptocurrency-news'\n",
    "    start = 1\n",
    "    get_page_cnt = 30\n",
    "    end = start + get_page_cnt\n",
    "\n",
    "    investing_results = []\n",
    "\n",
    "    for i in tqdm(range(start, end), mininterval=1, miniters=1):\n",
    "        result = sync_request(url=web_page, headers=headers)\n",
    "\n",
    "        if result['html'] is None:\n",
    "            print()\n",
    "            print(f'{i}번 페이지의 HTML 문서 정보를 불러오는데 실패했습니다.')\n",
    "            print(traceback.format_exc())\n",
    "            web_page = f'https://kr.investing.com/news/cryptocurrency-news/{i + 1}'\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(result['html'], 'html.parser')\n",
    "        url_tag_list = soup.find_all('article', {\"data-test\": \"article-item\"})\n",
    "        url_list = [url[\"href\"] for url_tag in url_tag_list if ((url := url_tag.find('a')) is not None)]\n",
    "        result = asyncio.run(async_main(url_list, category='암호화폐', website='investing', headers=headers))\n",
    "        investing_results.extend(result)\n",
    "\n",
    "        time.sleep(random.uniform(min_delay, max_delay))\n",
    "        web_page = f'https://kr.investing.com/news/cryptocurrency-news/{i + 1}'\n",
    "    \n",
    "    return investing_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def hankyung():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전역 변수 및 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# cpu 갯수\n",
    "workers = os.cpu_count()\n",
    "print(workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Agent 변경을 위한 옵션 설정\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "headers = {'User-Agent': user_agent}\n",
    "\n",
    "# client 파라미터\n",
    "follow_redirects = True # 리다이렉트 허용 여부\n",
    "timeout = 90 # 응답 대기 허용 시간\n",
    "encoding = 'utf-8'\n",
    "max_retry = 10 # HTML 문서 요청 최대 재시도 횟수\n",
    "min_delay = 0.55 # 재시도 할 때 딜레이의 최소 시간\n",
    "max_delay = 1.55 # 재시도 할 때 딜레이의 최대 시간\n",
    "\n",
    "# 경로\n",
    "investing_path = r'.\\datas\\news_data\\Investing_Data.json'\n",
    "hankyung_path = r'.\\datas\\news_data\\Hankyung_Data.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bycij_yb2IDB"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://kr.investing.com/news/cryptocurrency-news'\n",
    "\n",
    "# User-Agent 변경을 위한 옵션 설정\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "headers = {'User-Agent': user_agent}\n",
    "\n",
    "# client 파라미터\n",
    "follow_redirects = True # 리다이렉트 허용 여부\n",
    "timeout = 90 # 응답 대기 허용 시간\n",
    "encoding = 'utf-8'\n",
    "max_retry = 10 # HTML 문서 요청 최대 재시도 횟수\n",
    "min_delay = 0.55 # 재시도 할 때 딜레이의 최소 시간\n",
    "max_delay = 1.55 # 재시도 할 때 딜레이의 최대 시간\n",
    "\n",
    "# 경로\n",
    "investing_path = r'.\\datas\\news_data\\Investing_Data.json'\n",
    "hankyung_path = r'.\\datas\\news_data\\Hankyung_Data.json'\n",
    "\n",
    "response = sync_request(url=url, headers=headers, follow_redirects=follow_redirects, timeout=timeout, encoding=encoding,\n",
    "                        max_retry=max_retry, min_delay=min_delay, max_delay=max_delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://kr.investing.com/news/cryptocurrency-news'"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
