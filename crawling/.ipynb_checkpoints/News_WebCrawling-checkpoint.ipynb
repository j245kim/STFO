{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14345,
     "status": "ok",
     "timestamp": 1733586568195,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "9HokvFOG2ICx",
    "outputId": "d9c303a8-dc89-45cc-941a-e5b735d02dcd"
   },
   "outputs": [],
   "source": [
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10814,
     "status": "ok",
     "timestamp": 1733586579007,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "oKtWa7v02ICx",
    "outputId": "010a47b4-bff7-4ac7-e294-59fc0f062de2"
   },
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6792,
     "status": "ok",
     "timestamp": 1733586585797,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "2Zf1EfhFiiH9",
    "outputId": "6c49166f-fa78-4ce8-be90-231ebe539291"
   },
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcXucBjY20Ci"
   },
   "source": [
    "# 문제점\n",
    "\n",
    "1. 암호화폐 뉴스 사이트들 대부분이 서로 크롤링 하는 경우가 많아서 이미 해당 사건으로부터 몇 십분에서 몇 시간, 며칠이 지난 후에 뉴스가 올라오는 경우가 되게 많음\n",
    "\n",
    "예: A사이트는 B사이트를 크롤링해서 뉴스 게재 -> B사이트는 C사이트를 크롤링해서 뉴스 게재 -> C사이트는 A사이트를 크롤링해서 게재\n",
    "\n",
    "2. 원인 불명의 문제로 인해 크롤링할 때 일부 뉴스 데이터들이 누락"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4H1DYrC82ICy"
   },
   "source": [
    "# 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1733665110553,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "auvaotrw2ICz"
   },
   "outputs": [],
   "source": [
    "# 파이썬 표준 라이브러리\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from concurrent import futures\n",
    "\n",
    "# 파이썬 서드파티 라이브러리\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxDIDmyG2IDA"
   },
   "source": [
    "# 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_html(\n",
    "                url: str, headers: dict[str, str], allow_redirects: bool, timeout: int,\n",
    "                 max_retry: int\n",
    "                 ) -> str | None:\n",
    "    \"\"\"requests로 HTML 문서 정보를 불러오는 함수\n",
    "\n",
    "    Args:\n",
    "        url: URL\n",
    "        headers: 식별 정보\n",
    "        allow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "    \n",
    "    Return:\n",
    "        텍스트화한 HTML 문서 정보, str\n",
    "\n",
    "        or \n",
    "    \n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    html = None\n",
    "\n",
    "    for _ in range(max_retry):\n",
    "        # requests로 HTML GET\n",
    "        response = requests.get(url, headers=headers, allow_redirects=allow_redirects, timeout=timeout)\n",
    "        # HTML 문서 정보를 불러오는 것에 성공하면 for문 중단\n",
    "        if response.ok and response.status_code == requests.codes.ok:\n",
    "            html = response.text\n",
    "            break\n",
    "\n",
    "        time.sleep(random.uniform(0.5, 1.25))\n",
    "    \n",
    "    # 응답 요청이 실패했으면 메세지 출력\n",
    "    if html is None:\n",
    "        print()\n",
    "        print(response.reason)\n",
    "        print(f'HTML 문서 정보 가져오기를 실패한 URL : {url}')\n",
    "    \n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1733669946448,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "jdcV0NWCW0rV"
   },
   "outputs": [],
   "source": [
    "def investing(\n",
    "            news_url: str, headers: dict[str, str], allow_redirects: bool, timeout: int, max_retry: int,\n",
    "              p0: re.Pattern, p1: re.Pattern, p2: re.Pattern, p3: re.Pattern\n",
    "              ) -> dict[str, str, None] | None:\n",
    "    \"\"\"뉴스 URL을 바탕으로 크롤링을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        news_url_tag: 뉴스 URL\n",
    "        headers: 식별 정보\n",
    "        allow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        p0: \"입력: \\r\\n 2024- 12- 07- 오후 08:45\"와 같은 텍스트에서 ' \\r\\n '를 기준으로 분리하는 패턴\n",
    "        p1: \\n가 1개 이상 연속되는 패턴\n",
    "        p2: 디셉터에서 읽기 / Provided COINNESS / 이승훈 기자 123@gmail.com 등의 불필요한 텍스트 패턴\n",
    "        p3: 줄바꿈이 3번 이상 연속하면 줄바꿈 2번으로 보정\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"news_title\": 뉴스 제목, str\n",
    "            \"news_first_upload_time\": 뉴스 최초 업로드 시각, str | None\n",
    "            \"newsfinal_upload_time\": 뉴스 최종 수정 시각, str | None\n",
    "            \"author\": 뉴스 작성자, str | None\n",
    "            \"news_content\": 뉴스 본문, str\n",
    "            \"news_url\": 뉴스 URL, str\n",
    "            \"news_website\": 뉴스 웹사이트, str\n",
    "            \"note\": 비고, str | None\n",
    "        }\n",
    "\n",
    "        or\n",
    "\n",
    "        None\n",
    "    \"\"\"\n",
    "    info = {} # 뉴스 데이터 정보 Dictionary\n",
    "\n",
    "    # requests로 HTML GET\n",
    "    html = request_html(url=news_url, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "    # HTML 문서 정보를 불러오는 것에 실패하면 None 반환\n",
    "    if html is None:\n",
    "        return None\n",
    "    # BeautifulSoup로 parser\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 1. 뉴스 데이터의 제목\n",
    "    title = soup.find('h1', id='articleTitle')\n",
    "    title = title.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 2. 뉴스 데이터의 최초 업로드 시각과 최종 수정 시각\n",
    "    div = soup.find_all('div', {'class': 'flex flex-row items-center'})\n",
    "    span = div[1].find('span')\n",
    "    span = span.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    first_upload_time_list = p0.split(string=span)[1].split()\n",
    "    y_m_d = '-'.join(times[:-1] for times in first_upload_time_list[:3])\n",
    "    if first_upload_time_list[3] == '오전':\n",
    "        ap = 'AM'\n",
    "    else:\n",
    "        ap = 'PM'\n",
    "\n",
    "    first_upload_time = y_m_d + ' ' + ap + ' ' + first_upload_time_list[4]\n",
    "    last_upload_time = None\n",
    "\n",
    "\n",
    "    # 3. 뉴스 데이터의 기사 작성자\n",
    "    author = None\n",
    "\n",
    "    # 4. 뉴스 데이터의 본문\n",
    "    article = soup.find('div', id='article')\n",
    "\n",
    "    # td 태그들은 모두 제거\n",
    "    # a 태그인 텍스트들 중 p2 패턴이 있는 텍스트는 아래 데이터 전처리1에서 걸러지기 쉽게 앞/뒤로 \\n 추가\n",
    "    # li 태그 밑의 하위 태그들을 텍스트만 남기고 모두 제거\n",
    "    # em 태그의 시작에 \\n\\n을 추가\n",
    "    for tag in article.find_all(['td', 'a', 'li', 'em']):\n",
    "        match tag.name:\n",
    "            case 'td':\n",
    "                tag.decompose()\n",
    "            case 'a':\n",
    "                if p2.search(string=tag.text) is not None:\n",
    "                    tag.string = f'\\n{tag.text}\\n'\n",
    "                tag.unwrap()\n",
    "            case 'li':\n",
    "                text_content = ''.join(tag.stripped_strings)\n",
    "                tag.clear()  # 하위 태그 모두 제거\n",
    "                tag.string = f'\\n\\n-> {text_content}\\n\\n'  # 텍스트만 남김\n",
    "            case 'em':\n",
    "                tag.string = f'\\n\\n{tag.text}'\n",
    "\n",
    "    for p in article.find_all('p'):\n",
    "        p.string = p.text\n",
    "        if '투데이코리아' in p.text and '사진=' in p.text:\n",
    "            p.decompose()\n",
    "\n",
    "    # all_text = article.text.strip(' \\t\\n\\r\\f\\v')\n",
    "    all_text = article.get_text(separator='\\n', strip=True)\n",
    "    news_content = p1.split(string=all_text)\n",
    "    # 뉴스 데이터 본문의 데이터 전처리1\n",
    "    # 디셉터에서 읽기 / Provided COINNESS / 이승훈 기자 123@gmail.com 등의 불필요한 텍스트 제거\n",
    "    while news_content:\n",
    "        if  p2.search(string=news_content[-1]) is None:\n",
    "            break\n",
    "        del news_content[-1]\n",
    "        \n",
    "    # 뉴스 데이터 본문의 데이터 전처리2\n",
    "    # - 하나의 텍스트로 결합하되, 사이에 줄바꿈 2개 추가\n",
    "    # - 띄워쓰기 보정\n",
    "    # - 줄바꿈 보정1, 2\n",
    "    # - 맨 앞/뒤의 화이트스페이스(whitespace) 문자 제거\n",
    "    content = '\\n\\n'.join(news_content)\n",
    "    content = p3.sub(repl=r'\\n\\n', string=content)\n",
    "    content = content.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 5. 뉴스 웹사이트 이름\n",
    "    website = 'Investing'\n",
    "\n",
    "    # 6. 뉴스 카테고리\n",
    "    category = None\n",
    "    \n",
    "    # 7. 비고\n",
    "    note = None\n",
    "\n",
    "    info['news_title'] = title\n",
    "    info['news_first_upload_time'] = first_upload_time\n",
    "    info['news_last_upload_time'] = last_upload_time\n",
    "    info['author'] = author\n",
    "    info['news_content'] = content\n",
    "    info['news_url'] = news_url\n",
    "    info['news_website'] = website\n",
    "    info['news_category'] = category\n",
    "    info['note'] = note\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1733669946843,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "3MFKvcUcUq9_"
   },
   "outputs": [],
   "source": [
    "def hankyung(\n",
    "            news_url: str, headers: dict[str, str], allow_redirects: bool, timeout: int, max_retry: int,\n",
    "              p0: re.Pattern\n",
    "              ) -> dict[str, str, None] | None:\n",
    "    \"\"\"뉴스 URL을 바탕으로 크롤링을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        news_url_tag: 뉴스 URL\n",
    "        headers: 식별 정보\n",
    "        allow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        p0: 인도 벵갈루루=양한나 블루밍비트 기자 sheep@bloomingbit.io / (사진=연합뉴스)와같은 텍스트 패턴\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"news_title\": 뉴스 제목, str\n",
    "            \"news_first_upload_time\": 뉴스 최초 업로드 시각, str | None\n",
    "            \"newsfinal_upload_time\": 뉴스 최종 수정 시각, str | None\n",
    "            \"author\": 뉴스 작성자, str | None\n",
    "            \"news_content\": 뉴스 본문, str\n",
    "            \"news_url\": 뉴스 URL, str\n",
    "            \"news_website\": 뉴스 웹사이트, str\n",
    "            \"note\": 비고, str | None\n",
    "        }\n",
    "\n",
    "        or\n",
    "\n",
    "        None\n",
    "    \"\"\"\n",
    "    info = {} # 뉴스 데이터 정보 Dictionary\n",
    "\n",
    "    # requests로 HTML GET\n",
    "    html = request_html(url=news_url, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "    # HTML 문서 정보를 불러오는 것에 실패하면 None 반환\n",
    "    if html is None:\n",
    "        return None\n",
    "    # BeautifulSoup로 parser\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 1. 뉴스 데이터의 제목\n",
    "    title = soup.find('h1', {\"class\": \"headline\"})\n",
    "    title = title.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 2. 뉴스 데이터의 최초 업로드 시각과 최종 수정 시각\n",
    "    upload_times = soup.find_all('span', {\"class\": \"txt-date\"})\n",
    "\n",
    "    first_upload_time = upload_times[0].text\n",
    "    first_upload_time = datetime.strptime(first_upload_time, '%Y.%m.%d %H:%M')\n",
    "    first_upload_time = datetime.strftime(first_upload_time, '%Y-%m-%d %p %I:%M')\n",
    "    last_upload_time = upload_times[1].text\n",
    "    last_upload_time = datetime.strptime(last_upload_time, '%Y.%m.%d %H:%M')\n",
    "    last_upload_time = datetime.strftime(last_upload_time, '%Y-%m-%d %p %I:%M')\n",
    "\n",
    "    # 3. 뉴스 데이터의 기사 작성자\n",
    "    author_list = soup.find_all('div', {\"class\": \"author link subs_author_list\"})\n",
    "    author_list = map(lambda x: x.find(\"a\").text, author_list)\n",
    "    author = ', '.join(author_list)\n",
    "\n",
    "    # 4. 뉴스 데이터의 본문\n",
    "    articletxt = soup.find(\"div\", id=\"articletxt\")\n",
    "    while (strong_tag := articletxt.find(\"strong\")) is not None:\n",
    "        strong_tag.decompose()\n",
    "    while (figcaption_tag := articletxt.find(\"figcaption\")) is not None:\n",
    "        figcaption_tag.decompose()\n",
    "    text_list = articletxt.get_text(separator=\"\\n\", strip=True).split('\\n')\n",
    "    while text_list:\n",
    "        if p0.search(string=text_list[-1]) is None:\n",
    "            break\n",
    "        del text_list[-1]\n",
    "        \n",
    "    content = '\\n\\n'.join(text_list).strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 5. 뉴스 웹사이트 이름\n",
    "    website = 'Hankyung'\n",
    "\n",
    "    # 6. 뉴스 카테고리\n",
    "    category = None\n",
    "    \n",
    "    # 7. 비고\n",
    "    note = None\n",
    "\n",
    "    info['news_title'] = title\n",
    "    info['news_first_upload_time'] = first_upload_time\n",
    "    info['news_last_upload_time'] = last_upload_time\n",
    "    info['author'] = author\n",
    "    info['news_content'] = content\n",
    "    info['news_url'] = news_url\n",
    "    info['news_website'] = website\n",
    "    info['news_category'] = category\n",
    "    info['note'] = note\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1733669946843,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "ham3rmXQ2IDA"
   },
   "outputs": [],
   "source": [
    "def crawling(website: str, news_websites_dict: dict[str, str]):\n",
    "    \"\"\"Crawling을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        website: 웹사이트 이름, str\n",
    "        news_websites_dict: 웹사이트 Dictionary, dict[str, str]\n",
    "\n",
    "    Returns:\n",
    "        pass\n",
    "    \"\"\"\n",
    "\n",
    "    assert website in news_websites_dict, f'{website}은 대상 웹사이트가 아닙니다.'\n",
    "\n",
    "    match website:\n",
    "        case 'Investing':\n",
    "            pass\n",
    "\n",
    "        case 'Hankyung':\n",
    "            pass\n",
    "\n",
    "        case 'Bloomingbit':\n",
    "            pass\n",
    "\n",
    "        case 'Coinreaders':\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전역 변수 및 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# cpu 갯수\n",
    "workers = os.cpu_count()\n",
    "print(workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_websites_dict = {\n",
    "                        'Investing': 'https://kr.investing.com/news/cryptocurrency-news',\n",
    "                        'Hankyung': 'https://www.hankyung.com/koreamarket/news/crypto',\n",
    "                        'Bloomingbit': 'https://bloomingbit.io/feed',\n",
    "                        'Coinreaders': 'https://www.coinreaders.com/'\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Agent 변경을 위한 옵션 설정\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "headers = {'User-Agent': user_agent}\n",
    "\n",
    "# requests 파라미터\n",
    "allow_redirects = True # 리다이렉트 허용 여부\n",
    "timeout = 90 # 응답 대기 허용 시간\n",
    "max_retry = 10 # HTML 문서 요청 최대 재시도 횟수\n",
    "\n",
    "# 경로\n",
    "investing_path = r'C:\\Users\\User\\Desktop\\STFO_Project\\datas\\news_data\\Investing_Data.json'\n",
    "hankyung_path = r'C:\\Users\\User\\Desktop\\STFO_Project\\datas\\news_data\\Hankyung_Data.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bycij_yb2IDB"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwxuzzaF2IDC"
   },
   "source": [
    "## https://kr.investing.com/news/cryptocurrency-news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUegd-BSCjwG"
   },
   "source": [
    "### 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1263346,
     "status": "ok",
     "timestamp": 1733671553966,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "Ltv-G6ZTU_0p",
    "outputId": "1810bf2b-abe8-48f8-843d-dbf40bc19dc5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [03:07<00:00,  6.26s/it]\n"
     ]
    }
   ],
   "source": [
    "web_page = 'https://kr.investing.com/news/cryptocurrency-news'\n",
    "start = 1\n",
    "get_page_cnt = 30\n",
    "end = start + get_page_cnt\n",
    "p0 = re.compile(pattern=r'\\s+\\r\\n\\s+')\n",
    "p1 = re.compile(pattern=r'\\n+')\n",
    "p2 = re.compile(pattern=r'(읽기|provided|[a-z0-9]@[a-z0-9]|무단전재 및 재배포|이 글의 독자는 인베스팅프로 결제 시 쿠폰 코드|인베스팅닷컴 유저를 위한|지금 인베스팅프로)', flags=re.IGNORECASE)\n",
    "p3 = re.compile(pattern=r'([가-힣])\\.(\\w)')\n",
    "p4 = re.compile(pattern=r'\\.━')\n",
    "p5 = re.compile(pattern=r'\\n{3,}')\n",
    "# 일부 파라미터들을 고정한 investing 함수 생성\n",
    "fixed_investing = partial(investing,\n",
    "                            headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry,\n",
    "                            p0=p0, p1=p1, p2=p2, p3=p3, p4=p4, p5=p5)\n",
    "investing_results = []\n",
    "\n",
    "for i in tqdm(range(start, end), mininterval=1, miniters=1):\n",
    "    try:\n",
    "        html = request_html(url=web_page, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        url_tag_list = soup.find_all('article', {\"data-test\": \"article-item\"})\n",
    "        url_list = [url[\"href\"] for url_tag in url_tag_list if ((url := url_tag.find('a')) is not None)]\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'{i}번 페이지의 HTML 문서 정보를 불러오는데 실패했습니다.')\n",
    "        print(traceback.format_exc())\n",
    "        web_page = f'https://kr.investing.com/news/cryptocurrency-news/{i + 1}'\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "            data = executor.map(fixed_investing, url_list)\n",
    "\n",
    "        for idx, d in enumerate(data, start=1):\n",
    "            if d is not None:\n",
    "                investing_results.append(d)\n",
    "            else:\n",
    "                print()\n",
    "                print(f'-{i}번 페이지-')\n",
    "                print(f'{i}번 페이지의 {idx}번째 데이터를 가져오는 것에 실패했습니다.')\n",
    "                print(f'실패한 뉴스 데이터의 URL : {url_list[idx - 1]}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'-{i}번 페이지-')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    time.sleep(random.uniform(0.5, 1.25))\n",
    "    web_page = f'https://kr.investing.com/news/cryptocurrency-news/{i + 1}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVIrAc7ZCmQd"
   },
   "source": [
    "### JSON으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1733671674719,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "j1Hs6CWGcRYa"
   },
   "outputs": [],
   "source": [
    "with open(investing_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(investing_results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxW9SY3VjBWk"
   },
   "source": [
    "## https://www.hankyung.com/koreamarket/news/crypto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BX74tfLYnZ3S"
   },
   "source": [
    "### 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11688,
     "status": "ok",
     "timestamp": 1733670082440,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "nvQK7B2Fi06x",
    "outputId": "ae2f0f40-ca1b-4b38-ada7-8d69b3c577b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [11:42<00:00,  3.51s/it]\n"
     ]
    }
   ],
   "source": [
    "start = 1\n",
    "get_page_cnt = 200\n",
    "end = start + get_page_cnt\n",
    "p0 = re.compile(pattern=r'([a-z0-9]@[a-z0-9]|사진=|\\b기자\\b|영상촬영\\s*:|한국경제TV [가-힣]{2,4}입니다)', flags=re.IGNORECASE)\n",
    "\n",
    "# 일부 파라미터들을 고정한 hankyung 함수 생성\n",
    "fixed_hankyung = partial(hankyung,\n",
    "                            headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry,\n",
    "                            p0=p0)\n",
    "hankyung_results = []\n",
    "\n",
    "for i in tqdm(range(start, end)):\n",
    "    web_page = f'https://www.hankyung.com/koreamarket/news/crypto?page={i}'\n",
    "    try:\n",
    "        html = request_html(url=web_page, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        url_tag_list = soup.find_all('h2', {\"class\": \"news-tit\"})\n",
    "        url_list = [url[\"href\"] for url_tag in url_tag_list if ((url := url_tag.find('a')) is not None)]\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'{i}번 페이지의 HTML 문서 정보를 불러오는데 실패했습니다.')\n",
    "        print(traceback.format_exc())\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "            data = executor.map(fixed_hankyung, url_list)\n",
    "\n",
    "        for idx, d in enumerate(data, start=1):\n",
    "            if d is not None:\n",
    "                hankyung_results.append(d)\n",
    "            else:\n",
    "                print()\n",
    "                print(f'-{i}번 페이지-')\n",
    "                print(f'{i}번 페이지의 {idx}번째 데이터를 가져오는 것에 실패했습니다.')\n",
    "                print(f'실패한 뉴스 데이터의 URL : {url_list[idx - 1]}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'-{i}번 페이지-')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    time.sleep(random.uniform(0.5, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZb1FuoSZ0RJ"
   },
   "source": [
    "### JSON으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1733667830226,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "tF3M18pXZ0RK"
   },
   "outputs": [],
   "source": [
    "with open(hankyung_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(hankyung_results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가상자산 과세 유예가 불발되면 국내 거래 비중이 높은 김치코인 하방 압력이 커질 것이라는 우려가 나온다. 과세 시행으로 국내 거래소 이탈이 가속화되면 유동성 부족 현상이 심화될 것이라는 주장이다.\n",
      "\n",
      "29일 코인마켓캡에 따르면 국내 기업이 발행한 김치코인은 최근 전반적인 시장 호황에도 불구하고 최고가 대비 90% 이상 하락한 상태다. 김치코인 가운데 시가총액이 가장 높은 카이아(KAIA)의 경우 클레이튼(KLAY) 시절 기록한 최고가 4.38달러에 비해 96% 낮은 0.1935달러에 거래되고 있다. 카이아는 국내 블록체인 클레이튼과 핀시아를 통합해 출시한 메인넷으로, KAIA는 KLAY와 일대일 가치로 스왑(전환)됐다. 다른 김치코인도 상황은 마찬가지다. 테라클래식(LUNC)은 최고가 대비 -100%, 위믹스(WEMIX)도 -95% 떨어졌다. 이외에도 △아이콘(ICX) △보라(BORA) △밀크(MLK) 등 주요 김치코인 모두 최고가의 1/10 수준에 머물러있는 것으로 나타났다. 이는 미국 대선 이후 비트코인(BTC)과 솔라나(SOL) 등 주요 가상자산이 연일 신고가를 경신한 것과 대조하면 초라한 성적표다.\n",
      "\n",
      "설상가상으로 정부의 가상자산 과세 유예안이 불발될 가능성이 제기되면서 투자심리는 더욱 얼어붙었다. 과세가 시작되면 가상자산 거래량이 많은 ‘고래’들이 매매 수익의 22%에 달하는 세금을 피해 국내 거래소를 떠날 가능성이 높기 때문이다. 한 투자자는 “가상자산 거래내역을 경제협력개발기구(OECD) 국가 간 자동 공유하는 카프(CARF) 협정이 도입되는 2027년 이전에 일단 국내부터 과세를 시행하면, 고래 투자자 상당수가 해외 거래소로 이탈할 것\"이라며 “국내 거래소애서 주로 거래되는 김치코인 가격 지지력이 약화될 것”이라고 우려했다.\n",
      "\n",
      "실제로 김치코인은 국내 시장에 거래가 쏠려 있는 경향이 뚜렷하다. 대부분 업비트와 빗썸 등 국내 거래소에서 80% 이상의 거래량을 기록하고 있다. 바이낸스나 코인베이스 같은 주요 해외 거래소에 상장된 김치코인이 적고, 상장되더라도 글로벌 투자자들의 관심을 끌기 어렵기 때문이다. KAIA의 경우 바이낸스에 상장돼 있지만 여전히 국내 거래소에서 가장 많은 거래량이 나오고 있다. 29일 코인마켓캡 기준으로 빗썸·코인원에서의 거래량이 23.5%, 바이낸스 거래량이 22.45%다. ICX 역시 바이낸스 상장 종목이지만 업비트 거래량이 38.15%인 반면 바이낸스 거래량은 23%에 그쳤다.\n",
      "\n",
      "과세가 확정되면 김치코인 가격이 급등했다가 떨어지는 ‘펌프앤덤프’ 등 투자자 피해가 속출할 수 있다는 지적도 나온다. 과세 시행 전 시세조종 세력이 김치코인 가격을 일시적으로 올리고 매도물량을 개미 투자자들에게 떠넘길 수 있다는 것이다. 김치코인은 유동성이 낮고 시총 규모가 작기 때문에 이 같은 시세조종에 더욱 취약하다. 한국금융연구원의 지난해 보고서에 따르면 국내에서 주로 거래되는 김치코인 10개 중 9개에서 펌프앤덤프로 추정되는 사례가 발생했다.\n",
      "\n",
      "업계 관계자는 “가상자산 과세가 확정되면 세력 주도의 비정상적인 가격 급등으로 김치코인 도박장이 펼쳐질 것”이라며 “시세조종에 대한 처벌 규정을 담은 이용자보호법이 7월부터 시행되긴 했지만 이후에도 거래소들의 시세조종 방지 조치가 미흡한 모습을 보인 만큼 투자자 피해가 우려된다”고 지적했다.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://kr.investing.com/news/cryptocurrency-news/article-1286813'\n",
    "p0 = re.compile(pattern=r'\\s+\\r\\n\\s+')\n",
    "p1 = re.compile(pattern=r'\\n+')\n",
    "p2 = re.compile(pattern=r'(읽기|provided|[a-z0-9]@[a-z0-9]|무단전재 및 재배포|이 글의 독자는 인베스팅프로 결제 시 쿠폰 코드|인베스팅닷컴 유저를 위한|지금 인베스팅프로)', flags=re.IGNORECASE)\n",
    "p3 = re.compile(pattern=r'\\n{3,}')\n",
    "# 일부 파라미터들을 고정한 investing 함수 생성\n",
    "fixed_investing = partial(investing,\n",
    "                            headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry,\n",
    "                            p0=p0, p1=p1, p2=p2, p3=p3)\n",
    "\n",
    "res = fixed_investing(url)\n",
    "print(res['news_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:project]",
   "language": "python",
   "name": "conda-env-project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
