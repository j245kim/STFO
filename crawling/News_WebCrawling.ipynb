{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14345,
     "status": "ok",
     "timestamp": 1733586568195,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "9HokvFOG2ICx",
    "outputId": "d9c303a8-dc89-45cc-941a-e5b735d02dcd"
   },
   "outputs": [],
   "source": [
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10814,
     "status": "ok",
     "timestamp": 1733586579007,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "oKtWa7v02ICx",
    "outputId": "010a47b4-bff7-4ac7-e294-59fc0f062de2"
   },
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6792,
     "status": "ok",
     "timestamp": 1733586585797,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "2Zf1EfhFiiH9",
    "outputId": "6c49166f-fa78-4ce8-be90-231ebe539291"
   },
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcXucBjY20Ci"
   },
   "source": [
    "# 문제점\n",
    "\n",
    "1. 암호화폐 뉴스 사이트들 대부분이 서로 크롤링 하는 경우가 많아서 이미 해당 사건으로부터 몇 십분에서 몇 시간, 며칠이 지난 후에 뉴스가 올라오는 경우가 되게 많음\n",
    "\n",
    "예: A사이트는 B사이트를 크롤링해서 뉴스 게재 -> B사이트는 C사이트를 크롤링해서 뉴스 게재 -> C사이트는 A사이트를 크롤링해서 게재\n",
    "\n",
    "2. 원인 불명의 문제로 인해 크롤링할 때 일부 뉴스 데이터들이 누락"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4H1DYrC82ICy"
   },
   "source": [
    "# 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1733665110553,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "auvaotrw2ICz"
   },
   "outputs": [],
   "source": [
    "# 파이썬 표준 라이브러리\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from concurrent import futures\n",
    "\n",
    "# 파이썬 서드파티 라이브러리\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxDIDmyG2IDA"
   },
   "source": [
    "# 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_html(\n",
    "                url: str, headers: dict[str, str], allow_redirects: bool, timeout: int,\n",
    "                 max_retry: int\n",
    "                 ) -> str | None:\n",
    "    \"\"\"requests로 HTML 문서 정보를 불러오는 함수\n",
    "\n",
    "    Args:\n",
    "        url: URL\n",
    "        headers: 식별 정보\n",
    "        allow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "    \n",
    "    Return:\n",
    "        텍스트화한 HTML 문서 정보, str\n",
    "\n",
    "        or \n",
    "    \n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    html = None\n",
    "\n",
    "    for _ in range(max_retry):\n",
    "        # requests로 HTML GET\n",
    "        response = requests.get(url, headers=headers, allow_redirects=allow_redirects, timeout=timeout)\n",
    "        # HTML 문서 정보를 불러오는 것에 성공하면 for문 중단\n",
    "        if response.ok and response.status_code == requests.codes.ok:\n",
    "            html = response.text\n",
    "            break\n",
    "\n",
    "        time.sleep(random.uniform(0.5, 1.25))\n",
    "    \n",
    "    # 응답 요청이 실패했으면 메세지 출력\n",
    "    if html is None:\n",
    "        print()\n",
    "        print(response.reason)\n",
    "        print(f'HTML 문서 정보 가져오기를 실패한 URL : {url}')\n",
    "    \n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1733669946448,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "jdcV0NWCW0rV"
   },
   "outputs": [],
   "source": [
    "def investing(\n",
    "            news_url: str, headers: dict[str, str], allow_redirects: bool, timeout: int, max_retry: int,\n",
    "              p0: re.Pattern, p1: re.Pattern, p2: re.Pattern, p3: re.Pattern, p4: re.Pattern, p5: re.Pattern\n",
    "              ) -> dict[str, str, None] | None:\n",
    "    \"\"\"뉴스 URL을 바탕으로 크롤링을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        news_url_tag: 뉴스 URL\n",
    "        headers: 식별 정보\n",
    "        allow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        p0: \"입력: \\r\\n 2024- 12- 07- 오후 08:45\"와 같은 텍스트에서 ' \\r\\n '를 기준으로 분리하는 패턴\n",
    "        p1: \\n가 1개 이상 연속되는 패턴\n",
    "        p2: 디셉터에서 읽기 / Provided COINNESS / 이승훈 기자 123@gmail.com 등의 불필요한 텍스트 패턴\n",
    "        p3: \"습니다.그 결과,\"와 같은 텍스트에서 마침표 다음 띄워쓰기를 보정\n",
    "        p4: \"필요하다.━AI로\"와 같은 텍스트에서 마침표 다음 줄바꿈을 보정\n",
    "        p5: 줄바꿈이 3번 이상 연속하는 패턴\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"news_title\": 뉴스 제목, str\n",
    "            \"news_first_upload_time\": 뉴스 최초 업로드 시각, str | None\n",
    "            \"newsfinal_upload_time\": 뉴스 최종 수정 시각, str | None\n",
    "            \"author\": 뉴스 작성자, str | None\n",
    "            \"news_content\": 뉴스 본문, str\n",
    "            \"news_url\": 뉴스 URL, str\n",
    "            \"news_website\": 뉴스 웹사이트, str\n",
    "            \"note\": 비고, str | None\n",
    "        }\n",
    "\n",
    "        or\n",
    "\n",
    "        None\n",
    "    \"\"\"\n",
    "    info = {} # 뉴스 데이터 정보 Dictionary\n",
    "\n",
    "    # requests로 HTML GET\n",
    "    html = request_html(url=news_url, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "    # HTML 문서 정보를 불러오는 것에 실패하면 None 반환\n",
    "    if html is None:\n",
    "        return None\n",
    "    # BeautifulSoup로 parser\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 1. 뉴스 데이터의 제목\n",
    "    title = soup.find('h1', id='articleTitle')\n",
    "    title = title.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 2. 뉴스 데이터의 최초 업로드 시각과 최종 수정 시각\n",
    "    div = soup.find_all('div', {'class': 'flex flex-row items-center'})\n",
    "    span = div[1].find('span')\n",
    "    span = span.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    first_upload_time_list = p0.split(string=span)[1].split()\n",
    "    y_m_d = '-'.join(times[:-1] for times in first_upload_time_list[:3])\n",
    "    if first_upload_time_list[3] == '오전':\n",
    "        ap = 'AM'\n",
    "    else:\n",
    "        ap = 'PM'\n",
    "\n",
    "    first_upload_time = y_m_d + ' ' + ap + ' ' + first_upload_time_list[4]\n",
    "    last_upload_time = None\n",
    "\n",
    "\n",
    "    # 3. 뉴스 데이터의 기사 작성자\n",
    "    author = None\n",
    "\n",
    "    # 4. 뉴스 데이터의 본문\n",
    "    article = soup.find('div', id='article')\n",
    "    while (td_tag := article.find(\"td\")) is not None:\n",
    "        td_tag.decompose()\n",
    "    \n",
    "    # li 태그 밑의 하위 태그들을 텍스트만 남기고 모두 제거\n",
    "    for li in article.find_all('li'):\n",
    "        text_content = ''.join(li.stripped_strings)\n",
    "        li.clear()  # 하위 태그 모두 제거\n",
    "        li.string = f'\\n\\n-> {text_content}\\n\\n'  # 텍스트만 남김\n",
    "    \n",
    "    # em 태그의 시작에 \\n\\n을 추가\n",
    "    if (em_tag := article.find('em')) is not None:\n",
    "        em_tag.string = f'\\n\\n{em_tag.text}'\n",
    "    \n",
    "    all_text = article.text.strip(' \\t\\n\\r\\f\\v')\n",
    "    news_content = p1.split(string=all_text)\n",
    "    # 뉴스 데이터 본문의 데이터 전처리1\n",
    "    # 디셉터에서 읽기 / Provided COINNESS / 이승훈 기자 123@gmail.com 등의 불필요한 텍스트 제거\n",
    "    while news_content:\n",
    "        if  p2.search(string=news_content[-1]) is None:\n",
    "            break\n",
    "        del news_content[-1]\n",
    "        \n",
    "    # 뉴스 데이터 본문의 데이터 전처리2\n",
    "    # - 하나의 텍스트로 결합하되, 사이에 줄바꿈 2개 추가\n",
    "    # - 띄워쓰기 보정\n",
    "    # - 줄바꿈 보정1, 2\n",
    "    # - 맨 앞/뒤의 화이트스페이스(whitespace) 문자 제거\n",
    "    content = '\\n\\n'.join(news_content)\n",
    "    content = p3.sub(repl=r'\\g<1>. \\g<2>', string=content)\n",
    "    content = p4.sub(repl=r'.\\n\\n━', string=content)\n",
    "    content = p5.sub(repl=r'\\n\\n', string=content)\n",
    "    content = content.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 5. 뉴스 웹사이트 이름\n",
    "    website = 'Investing'\n",
    "\n",
    "    # 6. 뉴스 카테고리\n",
    "    category = None\n",
    "    \n",
    "    # 7. 비고\n",
    "    note = None\n",
    "\n",
    "    info['news_title'] = title\n",
    "    info['news_first_upload_time'] = first_upload_time\n",
    "    info['news_last_upload_time'] = last_upload_time\n",
    "    info['author'] = author\n",
    "    info['news_content'] = content\n",
    "    info['news_url'] = news_url\n",
    "    info['news_website'] = website\n",
    "    info['news_category'] = category\n",
    "    info['note'] = note\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1733669946843,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "3MFKvcUcUq9_"
   },
   "outputs": [],
   "source": [
    "def hankyung(\n",
    "            news_url: str, headers: dict[str, str], allow_redirects: bool, timeout: int, max_retry: int,\n",
    "              p0: re.Pattern\n",
    "              ) -> dict[str, str, None] | None:\n",
    "    \"\"\"뉴스 URL을 바탕으로 크롤링을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        news_url_tag: 뉴스 URL\n",
    "        headers: 식별 정보\n",
    "        allow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        p0: 인도 벵갈루루=양한나 블루밍비트 기자 sheep@bloomingbit.io / (사진=연합뉴스)와같은 텍스트 패턴\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"news_title\": 뉴스 제목, str\n",
    "            \"news_first_upload_time\": 뉴스 최초 업로드 시각, str | None\n",
    "            \"newsfinal_upload_time\": 뉴스 최종 수정 시각, str | None\n",
    "            \"author\": 뉴스 작성자, str | None\n",
    "            \"news_content\": 뉴스 본문, str\n",
    "            \"news_url\": 뉴스 URL, str\n",
    "            \"news_website\": 뉴스 웹사이트, str\n",
    "            \"note\": 비고, str | None\n",
    "        }\n",
    "\n",
    "        or\n",
    "\n",
    "        None\n",
    "    \"\"\"\n",
    "    info = {} # 뉴스 데이터 정보 Dictionary\n",
    "\n",
    "    # requests로 HTML GET\n",
    "    html = request_html(url=news_url, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "    # HTML 문서 정보를 불러오는 것에 실패하면 None 반환\n",
    "    if html is None:\n",
    "        return None\n",
    "    # BeautifulSoup로 parser\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 1. 뉴스 데이터의 제목\n",
    "    title = soup.find('h1', {\"class\": \"headline\"})\n",
    "    title = title.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 2. 뉴스 데이터의 최초 업로드 시각과 최종 수정 시각\n",
    "    upload_times = soup.find_all('span', {\"class\": \"txt-date\"})\n",
    "\n",
    "    first_upload_time = upload_times[0].text\n",
    "    first_upload_time = datetime.strptime(first_upload_time, '%Y.%m.%d %H:%M')\n",
    "    first_upload_time = datetime.strftime(first_upload_time, '%Y-%m-%d %p %I:%M')\n",
    "    last_upload_time = upload_times[1].text\n",
    "    last_upload_time = datetime.strptime(last_upload_time, '%Y.%m.%d %H:%M')\n",
    "    last_upload_time = datetime.strftime(last_upload_time, '%Y-%m-%d %p %I:%M')\n",
    "\n",
    "    # 3. 뉴스 데이터의 기사 작성자\n",
    "    author_list = soup.find_all('div', {\"class\": \"author link subs_author_list\"})\n",
    "    author_list = map(lambda x: x.find(\"a\").text, author_list)\n",
    "    author = ', '.join(author_list)\n",
    "\n",
    "    # 4. 뉴스 데이터의 본문\n",
    "    articletxt = soup.find(\"div\", id=\"articletxt\")\n",
    "    while (strong_tag := articletxt.find(\"strong\")) is not None:\n",
    "        strong_tag.decompose()\n",
    "    while (figcaption_tag := articletxt.find(\"figcaption\")) is not None:\n",
    "        figcaption_tag.decompose()\n",
    "    text_list = articletxt.get_text(separator=\"\\n\", strip=True).split('\\n')\n",
    "    while text_list:\n",
    "        if p0.search(string=text_list[-1]) is None:\n",
    "            break\n",
    "        del text_list[-1]\n",
    "        \n",
    "    content = '\\n\\n'.join(text_list).strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 5. 뉴스 웹사이트 이름\n",
    "    website = 'Hankyung'\n",
    "\n",
    "    # 6. 뉴스 카테고리\n",
    "    category = None\n",
    "    \n",
    "    # 7. 비고\n",
    "    note = None\n",
    "\n",
    "    info['news_title'] = title\n",
    "    info['news_first_upload_time'] = first_upload_time\n",
    "    info['news_last_upload_time'] = last_upload_time\n",
    "    info['author'] = author\n",
    "    info['news_content'] = content\n",
    "    info['news_url'] = news_url\n",
    "    info['news_website'] = website\n",
    "    info['news_category'] = category\n",
    "    info['note'] = note\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1733669946843,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "ham3rmXQ2IDA"
   },
   "outputs": [],
   "source": [
    "def crawling(website: str, news_websites_dict: dict[str, str]):\n",
    "    \"\"\"Crawling을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        website: 웹사이트 이름, str\n",
    "        news_websites_dict: 웹사이트 Dictionary, dict[str, str]\n",
    "\n",
    "    Returns:\n",
    "        pass\n",
    "    \"\"\"\n",
    "\n",
    "    assert website in news_websites_dict, f'{website}은 대상 웹사이트가 아닙니다.'\n",
    "\n",
    "    match website:\n",
    "        case 'Investing':\n",
    "            pass\n",
    "\n",
    "        case 'Hankyung':\n",
    "            pass\n",
    "\n",
    "        case 'Bloomingbit':\n",
    "            pass\n",
    "\n",
    "        case 'Coinreaders':\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전역 변수 및 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# cpu 갯수\n",
    "workers = os.cpu_count()\n",
    "print(workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_websites_dict = {\n",
    "                        'Investing': 'https://kr.investing.com/news/cryptocurrency-news',\n",
    "                        'Hankyung': 'https://www.hankyung.com/koreamarket/news/crypto',\n",
    "                        'Bloomingbit': 'https://bloomingbit.io/feed',\n",
    "                        'Coinreaders': 'https://www.coinreaders.com/'\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Agent 변경을 위한 옵션 설정\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "headers = {'User-Agent': user_agent}\n",
    "\n",
    "# requests 파라미터\n",
    "allow_redirects = True # 리다이렉트 허용 여부\n",
    "timeout = 90 # 응답 대기 허용 시간\n",
    "max_retry = 10 # HTML 문서 요청 최대 재시도 횟수\n",
    "\n",
    "# 경로\n",
    "investing_path = r'C:\\Users\\User\\Desktop\\STFO_Project\\datas\\news_data\\Investing_Data.json'\n",
    "hankyung_path = r'C:\\Users\\User\\Desktop\\STFO_Project\\datas\\news_data\\Hankyung_Data.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bycij_yb2IDB"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwxuzzaF2IDC"
   },
   "source": [
    "## https://kr.investing.com/news/cryptocurrency-news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUegd-BSCjwG"
   },
   "source": [
    "### 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1263346,
     "status": "ok",
     "timestamp": 1733671553966,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "Ltv-G6ZTU_0p",
    "outputId": "1810bf2b-abe8-48f8-843d-dbf40bc19dc5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [03:07<00:00,  6.26s/it]\n"
     ]
    }
   ],
   "source": [
    "web_page = 'https://kr.investing.com/news/cryptocurrency-news'\n",
    "start = 1\n",
    "get_page_cnt = 30\n",
    "end = start + get_page_cnt\n",
    "p0 = re.compile(pattern=r'\\s+\\r\\n\\s+')\n",
    "p1 = re.compile(pattern=r'\\n+')\n",
    "p2 = re.compile(pattern=r'(읽기|provided|[a-z0-9]@[a-z0-9]|무단전재 및 재배포|이 글의 독자는 인베스팅프로 결제 시 쿠폰 코드|인베스팅닷컴 유저를 위한|지금 인베스팅프로)', flags=re.IGNORECASE)\n",
    "p3 = re.compile(pattern=r'([가-힣])\\.(\\w)')\n",
    "p4 = re.compile(pattern=r'\\.━')\n",
    "p5 = re.compile(pattern=r'\\n{3,}')\n",
    "# 일부 파라미터들을 고정한 investing 함수 생성\n",
    "fixed_investing = partial(investing,\n",
    "                            headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry,\n",
    "                            p0=p0, p1=p1, p2=p2, p3=p3, p4=p4, p5=p5)\n",
    "investing_results = []\n",
    "\n",
    "for i in tqdm(range(start, end), mininterval=1, miniters=1):\n",
    "    try:\n",
    "        html = request_html(url=web_page, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        url_tag_list = soup.find_all('article', {\"data-test\": \"article-item\"})\n",
    "        url_list = [url[\"href\"] for url_tag in url_tag_list if ((url := url_tag.find('a')) is not None)]\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'{i}번 페이지의 HTML 문서 정보를 불러오는데 실패했습니다.')\n",
    "        print(traceback.format_exc())\n",
    "        web_page = f'https://kr.investing.com/news/cryptocurrency-news/{i + 1}'\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "            data = executor.map(fixed_investing, url_list)\n",
    "\n",
    "        for idx, d in enumerate(data, start=1):\n",
    "            if d is not None:\n",
    "                investing_results.append(d)\n",
    "            else:\n",
    "                print()\n",
    "                print(f'-{i}번 페이지-')\n",
    "                print(f'{i}번 페이지의 {idx}번째 데이터를 가져오는 것에 실패했습니다.')\n",
    "                print(f'실패한 뉴스 데이터의 URL : {url_list[idx - 1]}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'-{i}번 페이지-')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    time.sleep(random.uniform(0.5, 1.25))\n",
    "    web_page = f'https://kr.investing.com/news/cryptocurrency-news/{i + 1}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVIrAc7ZCmQd"
   },
   "source": [
    "### JSON으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1733671674719,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "j1Hs6CWGcRYa"
   },
   "outputs": [],
   "source": [
    "with open(investing_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(investing_results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxW9SY3VjBWk"
   },
   "source": [
    "## https://www.hankyung.com/koreamarket/news/crypto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BX74tfLYnZ3S"
   },
   "source": [
    "### 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11688,
     "status": "ok",
     "timestamp": 1733670082440,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "nvQK7B2Fi06x",
    "outputId": "ae2f0f40-ca1b-4b38-ada7-8d69b3c577b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [11:42<00:00,  3.51s/it]\n"
     ]
    }
   ],
   "source": [
    "start = 1\n",
    "get_page_cnt = 200\n",
    "end = start + get_page_cnt\n",
    "p0 = re.compile(pattern=r'([a-z0-9]@[a-z0-9]|사진=|\\b기자\\b|영상촬영\\s*:|한국경제TV [가-힣]{2,4}입니다)', flags=re.IGNORECASE)\n",
    "\n",
    "# 일부 파라미터들을 고정한 hankyung 함수 생성\n",
    "fixed_hankyung = partial(hankyung,\n",
    "                            headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry,\n",
    "                            p0=p0)\n",
    "hankyung_results = []\n",
    "\n",
    "for i in tqdm(range(start, end)):\n",
    "    web_page = f'https://www.hankyung.com/koreamarket/news/crypto?page={i}'\n",
    "    try:\n",
    "        html = request_html(url=web_page, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        url_tag_list = soup.find_all('h2', {\"class\": \"news-tit\"})\n",
    "        url_list = [url[\"href\"] for url_tag in url_tag_list if ((url := url_tag.find('a')) is not None)]\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'{i}번 페이지의 HTML 문서 정보를 불러오는데 실패했습니다.')\n",
    "        print(traceback.format_exc())\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "            data = executor.map(fixed_hankyung, url_list)\n",
    "\n",
    "        for idx, d in enumerate(data, start=1):\n",
    "            if d is not None:\n",
    "                hankyung_results.append(d)\n",
    "            else:\n",
    "                print()\n",
    "                print(f'-{i}번 페이지-')\n",
    "                print(f'{i}번 페이지의 {idx}번째 데이터를 가져오는 것에 실패했습니다.')\n",
    "                print(f'실패한 뉴스 데이터의 URL : {url_list[idx - 1]}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'-{i}번 페이지-')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    time.sleep(random.uniform(0.5, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZb1FuoSZ0RJ"
   },
   "source": [
    "### JSON으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1733667830226,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "tF3M18pXZ0RK"
   },
   "outputs": [],
   "source": [
    "with open(hankyung_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(hankyung_results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶19일(화): △전미주택건설업협회(NAHB) 주택가격지수 △오스틴 굴스비 연은 총재 연설 △美 10월 건축 허가 잠정치 △美 10월 주택착공건수\n"
     ]
    }
   ],
   "source": [
    "url = 'https://kr.investing.com/news/cryptocurrency-news/article-1275437'\n",
    "p0 = re.compile(pattern=r'\\s+\\r\\n\\s+')\n",
    "p1 = re.compile(pattern=r'\\n+')\n",
    "p2 = re.compile(pattern=r'(읽기|provided|[a-z0-9]@[a-z0-9]|무단전재 및 재배포|이 글의 독자는 인베스팅프로 결제 시 쿠폰 코드|인베스팅닷컴 유저를 위한|지금 인베스팅프로)', flags=re.IGNORECASE)\n",
    "p3 = re.compile(pattern=r'([가-힣])\\.(\\w)')\n",
    "p4 = re.compile(pattern=r'\\.━')\n",
    "p5 = re.compile(pattern=r'\\n{3,}')\n",
    "# 일부 파라미터들을 고정한 investing 함수 생성\n",
    "fixed_investing = partial(investing,\n",
    "                            headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry,\n",
    "                            p0=p0, p1=p1, p2=p2, p3=p3, p4=p4, p5=p5)\n",
    "\n",
    "res = fixed_investing(url)\n",
    "print(res['news_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
