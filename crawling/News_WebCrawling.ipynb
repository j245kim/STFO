{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14345,
     "status": "ok",
     "timestamp": 1733586568195,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "9HokvFOG2ICx",
    "outputId": "d9c303a8-dc89-45cc-941a-e5b735d02dcd"
   },
   "outputs": [],
   "source": [
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10814,
     "status": "ok",
     "timestamp": 1733586579007,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "oKtWa7v02ICx",
    "outputId": "010a47b4-bff7-4ac7-e294-59fc0f062de2"
   },
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6792,
     "status": "ok",
     "timestamp": 1733586585797,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "2Zf1EfhFiiH9",
    "outputId": "6c49166f-fa78-4ce8-be90-231ebe539291"
   },
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcXucBjY20Ci"
   },
   "source": [
    "# 문제점\n",
    "\n",
    "1. 암호화폐 뉴스 사이트들 대부분이 서로 크롤링 하는 경우가 많아서 이미 해당 사건으로부터 몇 십분에서 몇 시간, 며칠이 지난 후에 뉴스가 올라오는 경우가 되게 많음\n",
    "\n",
    "예: A사이트는 B사이트를 크롤링해서 뉴스 게재 -> B사이트는 C사이트를 크롤링해서 뉴스 게재 -> C사이트는 A사이트를 크롤링해서 게재\n",
    "\n",
    "2. 원인 불명의 문제로 인해 크롤링할 때 일부 뉴스 데이터들이 누락"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4H1DYrC82ICy"
   },
   "source": [
    "# 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1733665110553,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "auvaotrw2ICz"
   },
   "outputs": [],
   "source": [
    "# 파이썬 표준 라이브러리\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from concurrent import futures\n",
    "\n",
    "# 파이썬 서드파티 라이브러리\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxDIDmyG2IDA"
   },
   "source": [
    "# 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_html(\n",
    "                url: str, headers: dict[str, str], allow_redirects: bool, timeout: int,\n",
    "                 max_retry: int\n",
    "                 ) -> str | None:\n",
    "    \"\"\"requests로 HTML 문서 정보를 불러오는 함수\n",
    "\n",
    "    Args:\n",
    "        url: URL\n",
    "        headers: 식별 정보\n",
    "        allow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "    \n",
    "    Return:\n",
    "        텍스트화한 HTML 문서 정보, str\n",
    "\n",
    "        or \n",
    "    \n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    html = None\n",
    "\n",
    "    for _ in range(max_retry):\n",
    "        # requests로 HTML GET\n",
    "        response = requests.get(url, headers=headers, allow_redirects=allow_redirects, timeout=timeout)\n",
    "        # HTML 문서 정보를 불러오는 것에 성공하면 for문 중단\n",
    "        if response.ok and response.status_code == requests.codes.ok:\n",
    "            html = response.text\n",
    "            break\n",
    "\n",
    "        time.sleep(random.uniform(0.5, 1.25))\n",
    "    \n",
    "    # 응답 요청이 실패했으면 메세지 출력\n",
    "    if html is None:\n",
    "        print()\n",
    "        print(response.reason)\n",
    "        print(f'HTML 문서 정보 가져오기를 실패한 URL : {url}')\n",
    "    \n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1733669946448,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "jdcV0NWCW0rV"
   },
   "outputs": [],
   "source": [
    "def investing(\n",
    "            news_url: str, headers: dict[str, str], allow_redirects: bool, timeout: int, max_retry: int,\n",
    "              p0: re.Pattern, p1: re.Pattern, p2: re.Pattern, p3: re.Pattern, p4: re.Pattern, p5: re.Pattern\n",
    "              ) -> dict[str, str, None] | None:\n",
    "    \"\"\"뉴스 URL을 바탕으로 크롤링을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        news_url_tag: 뉴스 URL\n",
    "        headers: 식별 정보\n",
    "        allow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        p0: \"입력: \\r\\n 2024- 12- 07- 오후 08:45\"와 같은 텍스트에서 ' \\r\\n '를 기준으로 분리하는 패턴\n",
    "        p1: \\n가 1개 이상 연속되는 패턴\n",
    "        p2: 디셉터에서 읽기 / Provided COINNESS / 이승훈 기자 123@gmail.com 등의 불필요한 텍스트 패턴\n",
    "        p3: \"습니다.그 결과,\"와 같은 텍스트에서 마침표 다음 띄워쓰기를 보정\n",
    "        p4: \"필요하다.━AI로\"와 같은 텍스트에서 마침표 다음 줄바꿈을 보정\n",
    "        p5: 줄바꿈이 3번 이상 연속하면 줄바꿈 2번으로 보정\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"news_title\": 뉴스 제목, str\n",
    "            \"news_first_upload_time\": 뉴스 최초 업로드 시각, str | None\n",
    "            \"newsfinal_upload_time\": 뉴스 최종 수정 시각, str | None\n",
    "            \"author\": 뉴스 작성자, str | None\n",
    "            \"news_content\": 뉴스 본문, str\n",
    "            \"news_url\": 뉴스 URL, str\n",
    "            \"news_website\": 뉴스 웹사이트, str\n",
    "            \"note\": 비고, str | None\n",
    "        }\n",
    "\n",
    "        or\n",
    "\n",
    "        None\n",
    "    \"\"\"\n",
    "    info = {} # 뉴스 데이터 정보 Dictionary\n",
    "\n",
    "    # requests로 HTML GET\n",
    "    html = request_html(url=news_url, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "    # HTML 문서 정보를 불러오는 것에 실패하면 None 반환\n",
    "    if html is None:\n",
    "        return None\n",
    "    # BeautifulSoup로 parser\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 1. 뉴스 데이터의 제목\n",
    "    title = soup.find('h1', id='articleTitle')\n",
    "    title = title.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 2. 뉴스 데이터의 최초 업로드 시각과 최종 수정 시각\n",
    "    div = soup.find_all('div', {'class': 'flex flex-row items-center'})\n",
    "    span = div[1].find('span')\n",
    "    span = span.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    first_upload_time_list = p0.split(string=span)[1].split()\n",
    "    y_m_d = '-'.join(times[:-1] for times in first_upload_time_list[:3])\n",
    "    if first_upload_time_list[3] == '오전':\n",
    "        ap = 'AM'\n",
    "    else:\n",
    "        ap = 'PM'\n",
    "\n",
    "    first_upload_time = y_m_d + ' ' + ap + ' ' + first_upload_time_list[4]\n",
    "    last_upload_time = None\n",
    "\n",
    "\n",
    "    # 3. 뉴스 데이터의 기사 작성자\n",
    "    author = None\n",
    "\n",
    "    # 4. 뉴스 데이터의 본문\n",
    "    article = soup.find('div', id='article')\n",
    "\n",
    "    # td 태그들은 모두 제거\n",
    "    # a 태그인 텍스트들 중 p2 패턴이 있는 텍스트는 아래 데이터 전처리1에서 걸러지기 쉽게 앞/뒤로 \\n 추가\n",
    "    # li 태그 밑의 하위 태그들을 텍스트만 남기고 모두 제거\n",
    "    # em 태그의 시작에 \\n\\n을 추가\n",
    "    for tag in article.find_all(['td', 'a', 'li', 'em']):\n",
    "        match tag.name:\n",
    "            case 'td':\n",
    "                tag.decompose()\n",
    "            case 'a':\n",
    "                if p2.search(string=tag.text) is not None:\n",
    "                    tag.string = f'\\n{tag.text}\\n'\n",
    "                tag.unwrap()\n",
    "            case 'li':\n",
    "                text_content = ''.join(tag.stripped_strings)\n",
    "                tag.clear()  # 하위 태그 모두 제거\n",
    "                tag.string = f'\\n\\n-> {text_content}\\n\\n'  # 텍스트만 남김\n",
    "            case 'em':\n",
    "                tag.string = f'\\n\\n{tag.text}'\n",
    "    \n",
    "    all_text = article.text.strip(' \\t\\n\\r\\f\\v')\n",
    "    news_content = p1.split(string=all_text)\n",
    "    # 뉴스 데이터 본문의 데이터 전처리1\n",
    "    # 디셉터에서 읽기 / Provided COINNESS / 이승훈 기자 123@gmail.com 등의 불필요한 텍스트 제거\n",
    "    while news_content:\n",
    "        if  p2.search(string=news_content[-1]) is None:\n",
    "            break\n",
    "        del news_content[-1]\n",
    "        \n",
    "    # 뉴스 데이터 본문의 데이터 전처리2\n",
    "    # - 하나의 텍스트로 결합하되, 사이에 줄바꿈 2개 추가\n",
    "    # - 띄워쓰기 보정\n",
    "    # - 줄바꿈 보정1, 2\n",
    "    # - 맨 앞/뒤의 화이트스페이스(whitespace) 문자 제거\n",
    "    content = '\\n\\n'.join(news_content)\n",
    "    content = p3.sub(repl=r'\\g<1>. \\g<2>', string=content)\n",
    "    content = p4.sub(repl=r'\\g<1>\\n\\n━', string=content)\n",
    "    content = p5.sub(repl=r'\\n\\n', string=content)\n",
    "    content = content.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 5. 뉴스 웹사이트 이름\n",
    "    website = 'Investing'\n",
    "\n",
    "    # 6. 뉴스 카테고리\n",
    "    category = None\n",
    "    \n",
    "    # 7. 비고\n",
    "    note = None\n",
    "\n",
    "    info['news_title'] = title\n",
    "    info['news_first_upload_time'] = first_upload_time\n",
    "    info['news_last_upload_time'] = last_upload_time\n",
    "    info['author'] = author\n",
    "    info['news_content'] = content\n",
    "    info['news_url'] = news_url\n",
    "    info['news_website'] = website\n",
    "    info['news_category'] = category\n",
    "    info['note'] = note\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1733669946843,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "3MFKvcUcUq9_"
   },
   "outputs": [],
   "source": [
    "def hankyung(\n",
    "            news_url: str, headers: dict[str, str], allow_redirects: bool, timeout: int, max_retry: int,\n",
    "              p0: re.Pattern\n",
    "              ) -> dict[str, str, None] | None:\n",
    "    \"\"\"뉴스 URL을 바탕으로 크롤링을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        news_url_tag: 뉴스 URL\n",
    "        headers: 식별 정보\n",
    "        allow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        p0: 인도 벵갈루루=양한나 블루밍비트 기자 sheep@bloomingbit.io / (사진=연합뉴스)와같은 텍스트 패턴\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"news_title\": 뉴스 제목, str\n",
    "            \"news_first_upload_time\": 뉴스 최초 업로드 시각, str | None\n",
    "            \"newsfinal_upload_time\": 뉴스 최종 수정 시각, str | None\n",
    "            \"author\": 뉴스 작성자, str | None\n",
    "            \"news_content\": 뉴스 본문, str\n",
    "            \"news_url\": 뉴스 URL, str\n",
    "            \"news_website\": 뉴스 웹사이트, str\n",
    "            \"note\": 비고, str | None\n",
    "        }\n",
    "\n",
    "        or\n",
    "\n",
    "        None\n",
    "    \"\"\"\n",
    "    info = {} # 뉴스 데이터 정보 Dictionary\n",
    "\n",
    "    # requests로 HTML GET\n",
    "    html = request_html(url=news_url, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "    # HTML 문서 정보를 불러오는 것에 실패하면 None 반환\n",
    "    if html is None:\n",
    "        return None\n",
    "    # BeautifulSoup로 parser\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 1. 뉴스 데이터의 제목\n",
    "    title = soup.find('h1', {\"class\": \"headline\"})\n",
    "    title = title.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 2. 뉴스 데이터의 최초 업로드 시각과 최종 수정 시각\n",
    "    upload_times = soup.find_all('span', {\"class\": \"txt-date\"})\n",
    "\n",
    "    first_upload_time = upload_times[0].text\n",
    "    first_upload_time = datetime.strptime(first_upload_time, '%Y.%m.%d %H:%M')\n",
    "    first_upload_time = datetime.strftime(first_upload_time, '%Y-%m-%d %p %I:%M')\n",
    "    last_upload_time = upload_times[1].text\n",
    "    last_upload_time = datetime.strptime(last_upload_time, '%Y.%m.%d %H:%M')\n",
    "    last_upload_time = datetime.strftime(last_upload_time, '%Y-%m-%d %p %I:%M')\n",
    "\n",
    "    # 3. 뉴스 데이터의 기사 작성자\n",
    "    author_list = soup.find_all('div', {\"class\": \"author link subs_author_list\"})\n",
    "    author_list = map(lambda x: x.find(\"a\").text, author_list)\n",
    "    author = ', '.join(author_list)\n",
    "\n",
    "    # 4. 뉴스 데이터의 본문\n",
    "    articletxt = soup.find(\"div\", id=\"articletxt\")\n",
    "\n",
    "    # strong, figcaption 태그들을 모두 제거\n",
    "    for tag in articletxt.find_all(['strong', 'figcaption']):\n",
    "        tag.decompose()\n",
    "\n",
    "    text_list = articletxt.get_text(separator=\"\\n\", strip=True).split('\\n')\n",
    "    # \"사진=\", \"OOO 기자\" 등의 불필요한 텍스트 제거\n",
    "    while text_list:\n",
    "        if p0.search(string=text_list[-1]) is None:\n",
    "            break\n",
    "        del text_list[-1]\n",
    "        \n",
    "    content = '\\n\\n'.join(text_list).strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 5. 뉴스 웹사이트 이름\n",
    "    website = 'Hankyung'\n",
    "\n",
    "    # 6. 뉴스 카테고리\n",
    "    category = None\n",
    "    \n",
    "    # 7. 비고\n",
    "    note = None\n",
    "\n",
    "    info['news_title'] = title\n",
    "    info['news_first_upload_time'] = first_upload_time\n",
    "    info['news_last_upload_time'] = last_upload_time\n",
    "    info['author'] = author\n",
    "    info['news_content'] = content\n",
    "    info['news_url'] = news_url\n",
    "    info['news_website'] = website\n",
    "    info['news_category'] = category\n",
    "    info['note'] = note\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1733669946843,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "ham3rmXQ2IDA"
   },
   "outputs": [],
   "source": [
    "def crawling(website: str, news_websites_dict: dict[str, str]):\n",
    "    \"\"\"Crawling을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        website: 웹사이트 이름, str\n",
    "        news_websites_dict: 웹사이트 Dictionary, dict[str, str]\n",
    "\n",
    "    Returns:\n",
    "        pass\n",
    "    \"\"\"\n",
    "\n",
    "    assert website in news_websites_dict, f'{website}은 대상 웹사이트가 아닙니다.'\n",
    "\n",
    "    match website:\n",
    "        case 'Investing':\n",
    "            pass\n",
    "\n",
    "        case 'Hankyung':\n",
    "            pass\n",
    "\n",
    "        case 'Bloomingbit':\n",
    "            pass\n",
    "\n",
    "        case 'Coinreaders':\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전역 변수 및 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# cpu 갯수\n",
    "workers = os.cpu_count()\n",
    "print(workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_websites_dict = {\n",
    "                        'Investing': 'https://kr.investing.com/news/cryptocurrency-news',\n",
    "                        'Hankyung': 'https://www.hankyung.com/koreamarket/news/crypto',\n",
    "                        'Bloomingbit': 'https://bloomingbit.io/feed',\n",
    "                        'Coinreaders': 'https://www.coinreaders.com/'\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Agent 변경을 위한 옵션 설정\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "headers = {'User-Agent': user_agent}\n",
    "\n",
    "# requests 파라미터\n",
    "allow_redirects = True # 리다이렉트 허용 여부\n",
    "timeout = 90 # 응답 대기 허용 시간\n",
    "max_retry = 10 # HTML 문서 요청 최대 재시도 횟수\n",
    "\n",
    "# 경로\n",
    "investing_path = r'C:\\Users\\User\\Desktop\\STFO_Project\\datas\\news_data\\Investing_Data.json'\n",
    "hankyung_path = r'C:\\Users\\User\\Desktop\\STFO_Project\\datas\\news_data\\Hankyung_Data.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bycij_yb2IDB"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwxuzzaF2IDC"
   },
   "source": [
    "## https://kr.investing.com/news/cryptocurrency-news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUegd-BSCjwG"
   },
   "source": [
    "### 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1263346,
     "status": "ok",
     "timestamp": 1733671553966,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "Ltv-G6ZTU_0p",
    "outputId": "1810bf2b-abe8-48f8-843d-dbf40bc19dc5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [03:07<00:00,  6.26s/it]\n"
     ]
    }
   ],
   "source": [
    "web_page = 'https://kr.investing.com/news/cryptocurrency-news'\n",
    "start = 1\n",
    "get_page_cnt = 30\n",
    "end = start + get_page_cnt\n",
    "p0 = re.compile(pattern=r'\\s+\\r\\n\\s+')\n",
    "p1 = re.compile(pattern=r'\\n+')\n",
    "p2 = re.compile(pattern=r'(읽기|provided|[a-z0-9]@[a-z0-9]|무단전재 및 재배포|이 글의 독자는 인베스팅프로 결제 시 쿠폰 코드|인베스팅닷컴 유저를 위한|지금 인베스팅프로)', flags=re.IGNORECASE)\n",
    "p3 = re.compile(pattern=r'([가-힣])\\.(\\w)')\n",
    "p4 = re.compile(pattern=r'\\.━')\n",
    "p5 = re.compile(pattern=r'\\n{3,}')\n",
    "# 일부 파라미터들을 고정한 investing 함수 생성\n",
    "fixed_investing = partial(investing,\n",
    "                            headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry,\n",
    "                            p0=p0, p1=p1, p2=p2, p3=p3, p4=p4, p5=p5)\n",
    "investing_results = []\n",
    "\n",
    "for i in tqdm(range(start, end), mininterval=1, miniters=1):\n",
    "    try:\n",
    "        html = request_html(url=web_page, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        url_tag_list = soup.find_all('article', {\"data-test\": \"article-item\"})\n",
    "        url_list = [url[\"href\"] for url_tag in url_tag_list if ((url := url_tag.find('a')) is not None)]\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'{i}번 페이지의 HTML 문서 정보를 불러오는데 실패했습니다.')\n",
    "        print(traceback.format_exc())\n",
    "        web_page = f'https://kr.investing.com/news/cryptocurrency-news/{i + 1}'\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "            data = executor.map(fixed_investing, url_list)\n",
    "\n",
    "        for idx, d in enumerate(data, start=1):\n",
    "            if d is not None:\n",
    "                investing_results.append(d)\n",
    "            else:\n",
    "                print()\n",
    "                print(f'-{i}번 페이지-')\n",
    "                print(f'{i}번 페이지의 {idx}번째 데이터를 가져오는 것에 실패했습니다.')\n",
    "                print(f'실패한 뉴스 데이터의 URL : {url_list[idx - 1]}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'-{i}번 페이지-')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    time.sleep(random.uniform(0.5, 1.25))\n",
    "    web_page = f'https://kr.investing.com/news/cryptocurrency-news/{i + 1}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVIrAc7ZCmQd"
   },
   "source": [
    "### JSON으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1733671674719,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "j1Hs6CWGcRYa"
   },
   "outputs": [],
   "source": [
    "with open(investing_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(investing_results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxW9SY3VjBWk"
   },
   "source": [
    "## https://www.hankyung.com/koreamarket/news/crypto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BX74tfLYnZ3S"
   },
   "source": [
    "### 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11688,
     "status": "ok",
     "timestamp": 1733670082440,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "nvQK7B2Fi06x",
    "outputId": "ae2f0f40-ca1b-4b38-ada7-8d69b3c577b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [11:42<00:00,  3.51s/it]\n"
     ]
    }
   ],
   "source": [
    "start = 1\n",
    "get_page_cnt = 200\n",
    "end = start + get_page_cnt\n",
    "p0 = re.compile(pattern=r'([a-z0-9]@[a-z0-9]|사진=|\\b기자\\b|영상촬영\\s*:|한국경제TV [가-힣]{2,4}입니다)', flags=re.IGNORECASE)\n",
    "\n",
    "# 일부 파라미터들을 고정한 hankyung 함수 생성\n",
    "fixed_hankyung = partial(hankyung,\n",
    "                            headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry,\n",
    "                            p0=p0)\n",
    "hankyung_results = []\n",
    "\n",
    "for i in tqdm(range(start, end)):\n",
    "    web_page = f'https://www.hankyung.com/koreamarket/news/crypto?page={i}'\n",
    "    try:\n",
    "        html = request_html(url=web_page, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        url_tag_list = soup.find_all('h2', {\"class\": \"news-tit\"})\n",
    "        url_list = [url[\"href\"] for url_tag in url_tag_list if ((url := url_tag.find('a')) is not None)]\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'{i}번 페이지의 HTML 문서 정보를 불러오는데 실패했습니다.')\n",
    "        print(traceback.format_exc())\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "            data = executor.map(fixed_hankyung, url_list)\n",
    "\n",
    "        for idx, d in enumerate(data, start=1):\n",
    "            if d is not None:\n",
    "                hankyung_results.append(d)\n",
    "            else:\n",
    "                print()\n",
    "                print(f'-{i}번 페이지-')\n",
    "                print(f'{i}번 페이지의 {idx}번째 데이터를 가져오는 것에 실패했습니다.')\n",
    "                print(f'실패한 뉴스 데이터의 URL : {url_list[idx - 1]}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'-{i}번 페이지-')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    time.sleep(random.uniform(0.5, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZb1FuoSZ0RJ"
   },
   "source": [
    "### JSON으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1733667830226,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "tF3M18pXZ0RK"
   },
   "outputs": [],
   "source": [
    "with open(hankyung_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(hankyung_results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“’동일 활동, 동일 위험, 동일 규칙’ 원칙에 입각해 일관된 제도를 만듭니다. 자금세탁과 사이버 보안 등 규제 요건을 충족한다면 웹3 사업은 홍콩에서 번창할 수 있습니다.”\n",
      "\n",
      "━연말 스테이블코인 법안 제출, OTC·커스터디 검토\n",
      "\n",
      "지난 달 30일 홍콩 케리 호텔에서 열린 체인링크 스마트콘 행사장에서 만난 조셉 찬 홍콩 재무부 차관은 웹3와 핀테크가 미래라는 점을 강조하며 이같이 말했다. 찬 차관은 “웹3 산업이 지속 가능하고 책임감 있는 발전을 위해 명확하고 예측 가능한 규제가 필요하다고 설명했다. 이러한 기조에 따라 홍콩은 지난해 6월 가상자산 거래 플랫폼 라이선스 제도를 시행했고, 올해는 스테이블코인 관련 협의를 마쳤다. 그는 “올해 말까지 스테이블코인 규제 체계 법안을 입법회에 제출할 예정”이라면서 “가상자산 장외거래(OTC)와 가상자산 커스터디 서비스 제공업체에 대한 검토도 진행할 계획”이라고 전했다.\n",
      "\n",
      "이처럼 개방적 기조에도 홍콩의 가상자산 거래소에서 개인 투자자가 거래할 수 있는 가상자산은 비트코인(BTC), 이더리움(ETH), 체인링크(LINK), 아발란체(AVAX) 4종으로 제한된다. 100개가 넘는 가상자산이 거래되는 업비트, 빗썸 등 국내 주요 거래소와는 대조적이다. 이에 대해 찬 차관은 “홍콩 규제에선 전통 금융에서도 복잡하거나 공시가 적은 상품은 전문 투자자만 다룰 수 있다”면서 “가상자산에도 같은 원칙을 적용했을 뿐”이라고 답했다. 다만 그는 “이러한 상태가 영원히 지속되지는 않을 것”이라면서 제도 변화의 가능성을 시사했다.\n",
      "\n",
      "━글로벌 블록체인 참여…CBDC 등 규제 샌드박스 활발\n",
      "\n",
      "홍콩 정부의 일관된 태도는 정부가 시행하는 규제 샌드박스에서도 확인된다. 앱토스 랩스는 홍콩 대표 은행인 항셍은행, 보스턴 컨설팅 그룹과 함께 홍콩 중앙은행디지털화폐(CBDC) 파일럿 ‘프로젝트 e-HKD’에 참여하고 있다. 이들 그룹은 퍼블릭 블록체인에서 디지털 머니로 토큰화 펀드를 결제하는 방식을 시험할 예정이다. 애니모카브랜즈는 스탠다드차타드 홍콩, 홍콩 텔레콤과 함께 스테이블코인 발행 샌드박스 기업으로 선정됐다. 앱토스(APT)나 애니모카브랜즈 자체 토큰 모카(MOCA) 등 가상자산의 개인 투자자 거래는 금지해도, 다른 분야에서 기술력과 요건을 갖췄다면 정부 주도 사업에 참여할 기회를 제공하고 있는 것이다.\n",
      "\n",
      "이에 글로벌 블록체인 프로젝트도 홍콩 정부에 긍정적이다. 홍콩·싱가포르를 담당하는 아발란체 관계자는 “홍콩의 규제기관과 정부는 기저 기술을 이해하고 가치를 알아본다”면서 “중국 본토의 근접성 외에도 정부의 추진력, 기관과 기업의 웹3 기술 도입 준비 수준 등이 홍콩 진출의 동기가 됐다”고 설명했다. 아발란체는 최근 홍콩 실리콘 밸리로 불리는 사이버포트와 손잡고 현지 스타트업과 기업을 위한 웹3 혁신 프로그램을 출범했다. 100만 홍콩달러(약 1억 7090만 원) 규모의 혁신 펀딩 풀도 조성했다.\n",
      "\n",
      "찬 차관은 “홍콩의 핀테크·웹3 기업이 1100개를 넘어섰으며, 이는 1년 전과 비교해 약 15% 증가한 수치”라고 밝혔다. 특히 증가 기업의 약 60%가 해외 기업이라는 점에서 홍콩의 웹3 우호 정책이 실질적 성과를 거두고 있음을 보여준다. 그는 “더 많은 블록체인 기업이 홍콩에서 사업을 발전시키기를 바란다”고 힘주어 말했다.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://kr.investing.com/news/cryptocurrency-news/article-1263100'\n",
    "p0 = re.compile(pattern=r'\\s+\\r\\n\\s+')\n",
    "p1 = re.compile(pattern=r'\\n+')\n",
    "p2 = re.compile(pattern=r'(읽기|provided|[a-z0-9]@[a-z0-9]|무단전재 및 재배포|이 글의 독자는 인베스팅프로 결제 시 쿠폰 코드|인베스팅닷컴 유저를 위한|지금 인베스팅프로)', flags=re.IGNORECASE)\n",
    "p3 = re.compile(pattern=r'([가-힣])\\.(\\w)')\n",
    "p4 = re.compile(pattern=r'(.)━')\n",
    "p5 = re.compile(pattern=r'\\n{3,}')\n",
    "# 일부 파라미터들을 고정한 investing 함수 생성\n",
    "fixed_investing = partial(investing,\n",
    "                            headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry,\n",
    "                            p0=p0, p1=p1, p2=p2, p3=p3, p4=p4, p5=p5)\n",
    "\n",
    "res = fixed_investing(url)\n",
    "print(res['news_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
