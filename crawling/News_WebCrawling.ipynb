{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14345,
     "status": "ok",
     "timestamp": 1733586568195,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "9HokvFOG2ICx",
    "outputId": "d9c303a8-dc89-45cc-941a-e5b735d02dcd"
   },
   "outputs": [],
   "source": [
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10814,
     "status": "ok",
     "timestamp": 1733586579007,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "oKtWa7v02ICx",
    "outputId": "010a47b4-bff7-4ac7-e294-59fc0f062de2"
   },
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6792,
     "status": "ok",
     "timestamp": 1733586585797,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "2Zf1EfhFiiH9",
    "outputId": "6c49166f-fa78-4ce8-be90-231ebe539291"
   },
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcXucBjY20Ci"
   },
   "source": [
    "# 문제점\n",
    "\n",
    "1. 암호화폐 뉴스 사이트들 대부분이 서로 크롤링 하는 경우가 많아서 이미 해당 사건으로부터 몇 십분에서 몇 시간, 며칠이 지난 후에 뉴스가 올라오는 경우가 되게 많음\n",
    "\n",
    "예: A사이트는 B사이트를 크롤링해서 뉴스 게재 -> B사이트는 C사이트를 크롤링해서 뉴스 게재 -> C사이트는 A사이트를 크롤링해서 게재\n",
    "\n",
    "2. 원인 불명의 문제로 인해 크롤링할 때 일부 뉴스 데이터들이 누락"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4H1DYrC82ICy"
   },
   "source": [
    "# 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1733665110553,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "auvaotrw2ICz"
   },
   "outputs": [],
   "source": [
    "# 파이썬 표준 라이브러리\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from concurrent import futures\n",
    "\n",
    "# 파이썬 서드파티 라이브러리\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxDIDmyG2IDA"
   },
   "source": [
    "# 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_html(\n",
    "                url: str, headers: dict[str, str], allow_redirects: bool, timeout: int,\n",
    "                 max_retry: int\n",
    "                 ) -> str | None:\n",
    "    \"\"\"requests로 HTML 문서 정보를 불러오는 함수\n",
    "\n",
    "    Args:\n",
    "        url: URL\n",
    "        headers: 식별 정보\n",
    "        allow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "    \n",
    "    Return:\n",
    "        텍스트화한 HTML 문서 정보, str\n",
    "\n",
    "        or \n",
    "    \n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    html = None\n",
    "\n",
    "    for _ in range(max_retry):\n",
    "        # requests로 HTML GET\n",
    "        response = requests.get(url, headers=headers, allow_redirects=allow_redirects, timeout=timeout)\n",
    "        # HTML 문서 정보를 불러오는 것에 성공하면 for문 중단\n",
    "        if response.ok and response.status_code == requests.codes.ok:\n",
    "            html = response.text\n",
    "            break\n",
    "\n",
    "        time.sleep(random.uniform(0.5, 1.25))\n",
    "    \n",
    "    # 응답 요청이 실패했으면 메세지 출력\n",
    "    if html is None:\n",
    "        print()\n",
    "        print(response.reason)\n",
    "        print(f'HTML 문서 정보 가져오기를 실패한 URL : {url}')\n",
    "    \n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1733669946448,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "jdcV0NWCW0rV"
   },
   "outputs": [],
   "source": [
    "def investing(\n",
    "            news_url: str, headers: dict[str, str], allow_redirects: bool, timeout: int, max_retry: int,\n",
    "              p0: re.Pattern, p1: re.Pattern, p2: re.Pattern, p3: re.Pattern, p4: re.Pattern\n",
    "              ) -> dict[str, str, None] | None:\n",
    "    \"\"\"뉴스 URL을 바탕으로 크롤링을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        news_url_tag: 뉴스 URL\n",
    "        headers: 식별 정보\n",
    "        allow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        p0: \"입력: \\r\\n 2024- 12- 07- 오후 08:45\"와 같은 텍스트에서 ' \\r\\n '를 기준으로 분리하는 패턴\n",
    "        p1: \\n가 1개 이상 연속되는 패턴\n",
    "        p2: 디셉터에서 읽기 / Provided COINNESS / 이승훈 기자 123@gmail.com 등의 불필요한 텍스트 패턴\n",
    "        p3: (사진|출처)\\s*= 등의 불필요한 텍스트 패턴\n",
    "        p4: 줄바꿈이 3번 이상 연속하면 줄바꿈 2번으로 보정\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"news_title\": 뉴스 제목, str\n",
    "            \"news_first_upload_time\": 뉴스 최초 업로드 시각, str | None\n",
    "            \"newsfinal_upload_time\": 뉴스 최종 수정 시각, str | None\n",
    "            \"author\": 뉴스 작성자, str | None\n",
    "            \"news_content\": 뉴스 본문, str\n",
    "            \"news_url\": 뉴스 URL, str\n",
    "            \"news_website\": 뉴스 웹사이트, str\n",
    "            \"note\": 비고, str | None\n",
    "        }\n",
    "\n",
    "        or\n",
    "\n",
    "        None\n",
    "    \"\"\"\n",
    "    info = {} # 뉴스 데이터 정보 Dictionary\n",
    "\n",
    "    # requests로 HTML GET\n",
    "    html = request_html(url=news_url, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "    # HTML 문서 정보를 불러오는 것에 실패하면 None 반환\n",
    "    if html is None:\n",
    "        return None\n",
    "    # BeautifulSoup로 parser\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 1. 뉴스 데이터의 제목\n",
    "    title = soup.find('h1', id='articleTitle')\n",
    "    title = title.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 2. 뉴스 데이터의 최초 업로드 시각과 최종 수정 시각\n",
    "    div = soup.find_all('div', {'class': 'flex flex-row items-center'})\n",
    "    span = div[1].find('span')\n",
    "    span = span.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    first_upload_time_list = p0.split(string=span)[1].split()\n",
    "    y_m_d = '-'.join(times[:-1] for times in first_upload_time_list[:3])\n",
    "    if first_upload_time_list[3] == '오전':\n",
    "        ap = 'AM'\n",
    "    else:\n",
    "        ap = 'PM'\n",
    "\n",
    "    first_upload_time = y_m_d + ' ' + ap + ' ' + first_upload_time_list[4]\n",
    "    last_upload_time = None\n",
    "\n",
    "\n",
    "    # 3. 뉴스 데이터의 기사 작성자\n",
    "    author = None\n",
    "\n",
    "    # 4. 뉴스 데이터의 본문\n",
    "    article = soup.find('div', id='article')\n",
    "\n",
    "    # td 태그들은 모두 제거\n",
    "    # a 태그인 텍스트들 중 p2 패턴이 있는 텍스트는 아래 데이터 전처리1에서 걸러지기 쉽게 앞/뒤로 \\n 추가\n",
    "    # li 태그 밑의 하위 태그들을 텍스트만 남기고 모두 제거\n",
    "    # em 태그의 시작에 \\n\\n을 추가\n",
    "    for tag in article.find_all(['td', 'a', 'li', 'em']):\n",
    "        match tag.name:\n",
    "            case 'td':\n",
    "                tag.decompose()\n",
    "            case 'a':\n",
    "                if p2.search(string=tag.text) is not None:\n",
    "                    tag.string = f'\\n{tag.text}\\n'\n",
    "                tag.unwrap()\n",
    "            case 'li':\n",
    "                text_content = ''.join(tag.stripped_strings)\n",
    "                tag.clear()  # 하위 태그 모두 제거\n",
    "                tag.string = f'\\n\\n-> {text_content}\\n\\n'  # 텍스트만 남김\n",
    "            case 'em':\n",
    "                tag.string = f'\\n\\n{tag.text}'\n",
    "\n",
    "    for p in article.find_all('p'):\n",
    "        p.string = p.text\n",
    "        if p3.search(string=p.text) is not None:\n",
    "            p.decompose()\n",
    "    \n",
    "    for h2 in article.find_all('h2'):\n",
    "        h2.string = h2.text\n",
    "\n",
    "    all_text = article.get_text(separator='\\n', strip=True)\n",
    "    # all_text = article.text.strip(' \\t\\n\\r\\f\\v')\n",
    "    news_content = p1.split(string=all_text)\n",
    "    # 뉴스 데이터 본문의 데이터 전처리1\n",
    "    # 디셉터에서 읽기 / Provided COINNESS / 이승훈 기자 123@gmail.com 등의 불필요한 텍스트 제거\n",
    "    while news_content:\n",
    "        if  p2.search(string=news_content[-1]) is None:\n",
    "            break\n",
    "        del news_content[-1]\n",
    "        \n",
    "    # 뉴스 데이터 본문의 데이터 전처리2\n",
    "    # - 하나의 텍스트로 결합하되, 사이에 줄바꿈 2개 추가\n",
    "    # - 띄워쓰기 보정\n",
    "    # - 줄바꿈 보정1, 2\n",
    "    # - 맨 앞/뒤의 화이트스페이스(whitespace) 문자 제거\n",
    "    content = '\\n\\n'.join(news_content)\n",
    "    content = p4.sub(repl=r'\\n\\n', string=content)\n",
    "    content = content.replace('━\\n\\n', '━')\n",
    "    content = content.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 5. 뉴스 웹사이트 이름\n",
    "    website = 'Investing'\n",
    "\n",
    "    # 6. 뉴스 카테고리\n",
    "    category = None\n",
    "    \n",
    "    # 7. 비고\n",
    "    note = None\n",
    "\n",
    "    info['news_title'] = title\n",
    "    info['news_first_upload_time'] = first_upload_time\n",
    "    info['news_last_upload_time'] = last_upload_time\n",
    "    info['author'] = author\n",
    "    info['news_content'] = content\n",
    "    info['news_url'] = news_url\n",
    "    info['news_website'] = website\n",
    "    info['news_category'] = category\n",
    "    info['note'] = note\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1733669946843,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "3MFKvcUcUq9_"
   },
   "outputs": [],
   "source": [
    "def hankyung(\n",
    "            news_url: str, headers: dict[str, str], allow_redirects: bool, timeout: int, max_retry: int,\n",
    "              p0: re.Pattern\n",
    "              ) -> dict[str, str, None] | None:\n",
    "    \"\"\"뉴스 URL을 바탕으로 크롤링을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        news_url_tag: 뉴스 URL\n",
    "        headers: 식별 정보\n",
    "        allow_redirects: 리다이렉트 허용 여부\n",
    "        timeout: 응답 대기 허용 시간\n",
    "        max_retry: HTML 문서 정보 불러오기에 실패했을 때 재시도할 최대 횟수\n",
    "        p0: 인도 벵갈루루=양한나 블루밍비트 기자 sheep@bloomingbit.io / (사진=연합뉴스)와같은 텍스트 패턴\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"news_title\": 뉴스 제목, str\n",
    "            \"news_first_upload_time\": 뉴스 최초 업로드 시각, str | None\n",
    "            \"newsfinal_upload_time\": 뉴스 최종 수정 시각, str | None\n",
    "            \"author\": 뉴스 작성자, str | None\n",
    "            \"news_content\": 뉴스 본문, str\n",
    "            \"news_url\": 뉴스 URL, str\n",
    "            \"news_website\": 뉴스 웹사이트, str\n",
    "            \"note\": 비고, str | None\n",
    "        }\n",
    "\n",
    "        or\n",
    "\n",
    "        None\n",
    "    \"\"\"\n",
    "    info = {} # 뉴스 데이터 정보 Dictionary\n",
    "\n",
    "    # requests로 HTML GET\n",
    "    html = request_html(url=news_url, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "    # HTML 문서 정보를 불러오는 것에 실패하면 None 반환\n",
    "    if html is None:\n",
    "        return None\n",
    "    # BeautifulSoup로 parser\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 1. 뉴스 데이터의 제목\n",
    "    title = soup.find('h1', {\"class\": \"headline\"})\n",
    "    title = title.text.strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 2. 뉴스 데이터의 최초 업로드 시각과 최종 수정 시각\n",
    "    upload_times = soup.find_all('span', {\"class\": \"txt-date\"})\n",
    "\n",
    "    first_upload_time = upload_times[0].text\n",
    "    first_upload_time = datetime.strptime(first_upload_time, '%Y.%m.%d %H:%M')\n",
    "    first_upload_time = datetime.strftime(first_upload_time, '%Y-%m-%d %p %I:%M')\n",
    "    last_upload_time = upload_times[1].text\n",
    "    last_upload_time = datetime.strptime(last_upload_time, '%Y.%m.%d %H:%M')\n",
    "    last_upload_time = datetime.strftime(last_upload_time, '%Y-%m-%d %p %I:%M')\n",
    "\n",
    "    # 3. 뉴스 데이터의 기사 작성자\n",
    "    author_list = soup.find_all('div', {\"class\": \"author link subs_author_list\"})\n",
    "    author_list = map(lambda x: x.find(\"a\").text, author_list)\n",
    "    author = ', '.join(author_list)\n",
    "\n",
    "    # 4. 뉴스 데이터의 본문\n",
    "    articletxt = soup.find(\"div\", id=\"articletxt\")\n",
    "\n",
    "    # strong, figcaption 태그들을 모두 제거\n",
    "    for tag in articletxt.find_all([\"strong\", \"figcaption\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text_list = articletxt.get_text(separator=\"\\n\", strip=True).split('\\n')\n",
    "\n",
    "    # 사진= / 영상촬영 등의 불필요한 텍스트 제거\n",
    "    while text_list:\n",
    "        if p0.search(string=text_list[-1]) is None:\n",
    "            break\n",
    "        del text_list[-1]\n",
    "        \n",
    "    content = '\\n\\n'.join(text_list).strip(' \\t\\n\\r\\f\\v')\n",
    "\n",
    "    # 5. 뉴스 웹사이트 이름\n",
    "    website = 'Hankyung'\n",
    "\n",
    "    # 6. 뉴스 카테고리\n",
    "    category = None\n",
    "    \n",
    "    # 7. 비고\n",
    "    note = None\n",
    "\n",
    "    info['news_title'] = title\n",
    "    info['news_first_upload_time'] = first_upload_time\n",
    "    info['news_last_upload_time'] = last_upload_time\n",
    "    info['author'] = author\n",
    "    info['news_content'] = content\n",
    "    info['news_url'] = news_url\n",
    "    info['news_website'] = website\n",
    "    info['news_category'] = category\n",
    "    info['note'] = note\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1733669946843,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "ham3rmXQ2IDA"
   },
   "outputs": [],
   "source": [
    "def crawling(website: str, news_websites_dict: dict[str, str]):\n",
    "    \"\"\"Crawling을 하는 함수\n",
    "\n",
    "    Args:\n",
    "        website: 웹사이트 이름, str\n",
    "        news_websites_dict: 웹사이트 Dictionary, dict[str, str]\n",
    "\n",
    "    Returns:\n",
    "        pass\n",
    "    \"\"\"\n",
    "\n",
    "    assert website in news_websites_dict, f'{website}은 대상 웹사이트가 아닙니다.'\n",
    "\n",
    "    match website:\n",
    "        case 'Investing':\n",
    "            pass\n",
    "\n",
    "        case 'Hankyung':\n",
    "            pass\n",
    "\n",
    "        case 'Bloomingbit':\n",
    "            pass\n",
    "\n",
    "        case 'Coinreaders':\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전역 변수 및 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# cpu 갯수\n",
    "workers = os.cpu_count()\n",
    "print(workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_websites_dict = {\n",
    "                        'Investing': 'https://kr.investing.com/news/cryptocurrency-news',\n",
    "                        'Hankyung': 'https://www.hankyung.com/koreamarket/news/crypto',\n",
    "                        'Bloomingbit': 'https://bloomingbit.io/feed',\n",
    "                        'Coinreaders': 'https://www.coinreaders.com/'\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Agent 변경을 위한 옵션 설정\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "headers = {'User-Agent': user_agent}\n",
    "\n",
    "# requests 파라미터\n",
    "allow_redirects = True # 리다이렉트 허용 여부\n",
    "timeout = 90 # 응답 대기 허용 시간\n",
    "max_retry = 10 # HTML 문서 요청 최대 재시도 횟수\n",
    "\n",
    "# 경로\n",
    "investing_path = r'C:\\Users\\User\\Desktop\\STFO_Project\\datas\\news_data\\Investing_Data.json'\n",
    "hankyung_path = r'C:\\Users\\User\\Desktop\\STFO_Project\\datas\\news_data\\Hankyung_Data.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bycij_yb2IDB"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwxuzzaF2IDC"
   },
   "source": [
    "## https://kr.investing.com/news/cryptocurrency-news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUegd-BSCjwG"
   },
   "source": [
    "### 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1263346,
     "status": "ok",
     "timestamp": 1733671553966,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "Ltv-G6ZTU_0p",
    "outputId": "1810bf2b-abe8-48f8-843d-dbf40bc19dc5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [03:07<00:00,  6.26s/it]\n"
     ]
    }
   ],
   "source": [
    "web_page = 'https://kr.investing.com/news/cryptocurrency-news'\n",
    "start = 1\n",
    "get_page_cnt = 30\n",
    "end = start + get_page_cnt\n",
    "p0 = re.compile(pattern=r'\\s+\\r\\n\\s+')\n",
    "p1 = re.compile(pattern=r'\\n+')\n",
    "p2 = re.compile(pattern=r'(읽기|provided|[a-z0-9]@[a-z0-9]|무단전재 및 재배포|이 글의 독자는 인베스팅프로 결제 시 쿠폰 코드|인베스팅닷컴 유저를 위한|지금 인베스팅프로)', flags=re.IGNORECASE)\n",
    "p3 = re.compile(pattern=r'([가-힣])\\.(\\w)')\n",
    "p4 = re.compile(pattern=r'\\.━')\n",
    "p5 = re.compile(pattern=r'\\n{3,}')\n",
    "# 일부 파라미터들을 고정한 investing 함수 생성\n",
    "fixed_investing = partial(investing,\n",
    "                            headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry,\n",
    "                            p0=p0, p1=p1, p2=p2, p3=p3, p4=p4, p5=p5)\n",
    "investing_results = []\n",
    "\n",
    "for i in tqdm(range(start, end), mininterval=1, miniters=1):\n",
    "    try:\n",
    "        html = request_html(url=web_page, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        url_tag_list = soup.find_all('article', {\"data-test\": \"article-item\"})\n",
    "        url_list = [url[\"href\"] for url_tag in url_tag_list if ((url := url_tag.find('a')) is not None)]\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'{i}번 페이지의 HTML 문서 정보를 불러오는데 실패했습니다.')\n",
    "        print(traceback.format_exc())\n",
    "        web_page = f'https://kr.investing.com/news/cryptocurrency-news/{i + 1}'\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "            data = executor.map(fixed_investing, url_list)\n",
    "\n",
    "        for idx, d in enumerate(data, start=1):\n",
    "            if d is not None:\n",
    "                investing_results.append(d)\n",
    "            else:\n",
    "                print()\n",
    "                print(f'-{i}번 페이지-')\n",
    "                print(f'{i}번 페이지의 {idx}번째 데이터를 가져오는 것에 실패했습니다.')\n",
    "                print(f'실패한 뉴스 데이터의 URL : {url_list[idx - 1]}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'-{i}번 페이지-')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    time.sleep(random.uniform(0.5, 1.25))\n",
    "    web_page = f'https://kr.investing.com/news/cryptocurrency-news/{i + 1}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVIrAc7ZCmQd"
   },
   "source": [
    "### JSON으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1733671674719,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "j1Hs6CWGcRYa"
   },
   "outputs": [],
   "source": [
    "with open(investing_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(investing_results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxW9SY3VjBWk"
   },
   "source": [
    "## https://www.hankyung.com/koreamarket/news/crypto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BX74tfLYnZ3S"
   },
   "source": [
    "### 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11688,
     "status": "ok",
     "timestamp": 1733670082440,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "nvQK7B2Fi06x",
    "outputId": "ae2f0f40-ca1b-4b38-ada7-8d69b3c577b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [11:42<00:00,  3.51s/it]\n"
     ]
    }
   ],
   "source": [
    "start = 1\n",
    "get_page_cnt = 200\n",
    "end = start + get_page_cnt\n",
    "p0 = re.compile(pattern=r'([a-z0-9]@[a-z0-9]|사진=|\\b기자\\b|영상촬영\\s*:|한국경제TV [가-힣]{2,4}입니다)', flags=re.IGNORECASE)\n",
    "\n",
    "# 일부 파라미터들을 고정한 hankyung 함수 생성\n",
    "fixed_hankyung = partial(hankyung,\n",
    "                            headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry,\n",
    "                            p0=p0)\n",
    "hankyung_results = []\n",
    "\n",
    "for i in tqdm(range(start, end)):\n",
    "    web_page = f'https://www.hankyung.com/koreamarket/news/crypto?page={i}'\n",
    "    try:\n",
    "        html = request_html(url=web_page, headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        url_tag_list = soup.find_all('h2', {\"class\": \"news-tit\"})\n",
    "        url_list = [url[\"href\"] for url_tag in url_tag_list if ((url := url_tag.find('a')) is not None)]\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'{i}번 페이지의 HTML 문서 정보를 불러오는데 실패했습니다.')\n",
    "        print(traceback.format_exc())\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "            data = executor.map(fixed_hankyung, url_list)\n",
    "\n",
    "        for idx, d in enumerate(data, start=1):\n",
    "            if d is not None:\n",
    "                hankyung_results.append(d)\n",
    "            else:\n",
    "                print()\n",
    "                print(f'-{i}번 페이지-')\n",
    "                print(f'{i}번 페이지의 {idx}번째 데이터를 가져오는 것에 실패했습니다.')\n",
    "                print(f'실패한 뉴스 데이터의 URL : {url_list[idx - 1]}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f'-{i}번 페이지-')\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    time.sleep(random.uniform(0.5, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZb1FuoSZ0RJ"
   },
   "source": [
    "### JSON으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1733667830226,
     "user": {
      "displayName": "Seung Hoon Lee",
      "userId": "06197787624635186839"
     },
     "user_tz": -540
    },
    "id": "tF3M18pXZ0RK"
   },
   "outputs": [],
   "source": [
    "with open(hankyung_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(hankyung_results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "※편집자 주 - 2017년부터 불어닥친 블록체인 열풍에 국내 주요 기업들도 잇따라 시장에 뛰어들었습니다. 수 년이 흐른 시점에서 디센터는  시리즈를 준비했습니다. 금융과 정보기술(IT)을 포함한 다양한 분야의 대기업들이 그동안 어떤 블록체인 전략을 펼쳤는지, 그리고 결과는 어땠는지 중간 점검한다는 취지입니다. 앞서의 시행착오와 성공 사례가 업계의 현재와 미래를 그려볼 수 있을 것입니다.\n",
      "\n",
      "준비 운동을 과하게 하면 본 경기에 들어가기도 전에 지친다. 네이버 관계사 라인은 올해로 6년째 블록체인 사업 초석 다지기에 공을 들이고 있다. 이제는 제대로 된 성과를 보여줄 시점이라다는 목소리가 나온다.\n",
      "\n",
      "━2018년 블록체인 사업 착수…거래소도 진출\n",
      "\n",
      "라인은 지난 2017년 12월 라인 플러스 내부에 블록체인 프로덕트 개발 조직 블록체인 랩을 신설했다. 이듬해 4월 토큰 이코노미 사업 조직 언블락을 세웠고, 두 달 뒤 블록체인 메인넷 개발 조직 언체인을 설립했다. 같은 해 8월 라인 블록체인이 출범했다.\n",
      "\n",
      "라인 블록체인의 네이티브 코인 링크(LN)는 라인 자회사 라인테크플러스가 발행했다. 라인테크플러스는 라인과 라인 자회사 LVC가 2018년 11월 싱가포르에 설립한 회사다. 당시 한창 암호화폐공개(ICO) 열풍이 불던 시점이었지만 라인은 독자 노선을 택했다. ICO 대신 생태계 참여자에게 보상으로 LN을 지급하는 방안을 구축했다. 라인은 LN을 유통할 거래소까지 직접 만들었다. 일본 가상자산 거래소 라인 비트맥스, 미국 기반 가상자산 거래소 비트프론트(구 비트박스)를 세웠다. LN은 초창기 이들 거래소에서만 유통됐다. 글로벌 거래소 상장을 위해 혈안이 된 다른 프로젝트와는 대조적인 행보를 보인 셈이다. 당시 라인은 디앱, 가상자산 지갑, 거래소까지 아우르는 토큰 이코노미를 구축하고자 했다. 대기업이기에 가능한 청사진이었다.\n",
      "\n",
      "그러나 폐쇄적 정책으로 사용자를 라인 생태계에 락인한다는 전략은 빛을 발하지 못했다. 카카오 클레이튼이 출범 당시 다양한 대기업의 합류를 발표하며 이목을 끌었던 데 비해 라인 블록체인은 상대적으로 주목을 받지 못했다. 거래소 사업도 난관에 부딪혔다. 비트프론트는 지난해 11월 사업을 접겠다고 공표한 데 이어 올 3월 폐업했다. 당시 김우석 라인테크플러스 대표는 라인 블록체인 커뮤니티를 통해 “비트프론트를 기반으로 뭔가 해보려고 했던 시기가 있었지만 실패했다”면서 “링크 사업을 확장하는 데 제약이 있었다”고 전했다.\n",
      "\n",
      "━“그간 개념검증…본격 사업은 지난해부터”\n",
      "\n",
      "반면 솔라나, 폴리곤 등은 라인 블록체인과 비슷한 시기 출범해 글로벌 프로젝트로 성장하는 데 성공했다. 올해 메인넷을 내놓은 수이와 앱토스도 단숨에 시장 선두주자로 올라섰다. 라인 블록체인은 운영 기간에 비해 글로벌 시장에서 입지가 약하다는 평가다. 9일 오후 1시 14분 코인마켓캡 기준 핀시아(구 링크) 가격은 전일 대비 5.18% 오른 29.84달러, 전체 암호화폐 시가총액 순위로는 236위를 기록했다. 최근 임직원의 횡령·배임 논란에 휩싸여 가격이 곤두박질 친 클레이튼(KLAY)도 시가총액 순위는 86위다. 이에 대해 라인 관계자는 “라인은 지난해까지 프라이빗 메인넷을 운영하며 사업에 대한 개념검증(PoC, Proof of Concept)을 진행해 왔다”며 “글로벌 월렛과 NFT 마켓도 지난해 9월 처음 베타 출시했다”고 전했다. “본격적 사업은 지난해 라인 넥스트를 설립하면서부터 추진했다”고도 덧붙였다. 라인은 지난 2021년 12월 NFT 기반 사업을 확장하기 위해 라인넥스트를 한국(LINE NEWT Corporation)과 미국(LINE NEXT Inc)에 세운 바 있다.\n",
      "\n",
      "━NFT 플랫폼 출범…성과는 아시아 한정\n",
      "\n",
      "약 5년에 걸쳐 개념검증을 진행했지만 여전히 성과는 아쉽다는 평가가 나온다. 라인넥스트는 지난해 9월 NFT 마켓 플레이스 ‘도시’의 베타 버전을 출시했다. 라인 관계자는 “그간 누적 거래는 41만 건 이상, 누적 거래자 수는 19만 명 이상, 멤버십 사용자는 약 460만 명을 기록했다”고 소개했다. 그는 도시가 “아시아에서 압도적 규모로 1위를 차지하고 있다”고도 전했다. 그러나 시야를 글로벌로 넓히면 규모는 미미한 수준이다. 댑레이더가 지난 3일 발표한 ‘2023년 10월 산업 리포트’에 따르면 지난 10월 한 달 간 NFT 판매 건수는 약 340만 건에 이른다. 도시에서 약 1년 간 거래된 건 수의 8배가 넘는 수치다. 다만 네이버 페이 등이 결제에서 차지하는 비중이 높다는 점은 주목할 만하다. 라인 관계자는 “1차 판매 거래 비중을 보면 네이버 페이, 신용카드 결제가 32%에 달해 대중 서비스 시장으로의 확장 가능성을 보여주고 있다”고 전했다.\n",
      "\n",
      "━핀시아로 이름 바꾸고 새출발 도모\n",
      "\n",
      "라인은 올해 라인 블록체인 이름을 핀시아로 바꾸고 블록체인 사업에 변화를 꾀했다. 라인테크플러스는 아부다비에 핀시아 재단을 세우고, 핀시아 체인과 관련된 운영 권한을 모두 넘겼다. 링크(LN)도 핀시아(FNSA)라는 이름으로 재탄생했다. 사전 예비 물량을 발행하지 않는 ‘제로 리저브’ 정책도 대대적으로 공시했다. 핀시아 재단 관계자는 “핀시아 재단이나 라인 모두 토큰을 레버리지해 수익화하는 구조가 없다는 의미”라고 설명했다.\n",
      "\n",
      "━핀시아 재단·라인 역할 분담…웹3 대중화 노린다\n",
      "\n",
      "최근 핀시아 재단은 웹3의 완전한 대중화를 목표로 속도를 내고 있다. 지난 달 핀시아 메인넷 2.0 업그레이드를 완료하고 퍼블릭 체인으로 전환했다. 소프트뱅크, 라인넥스트, 에이포티원(A41), 안랩블록체인컴퍼니 등으로 이뤄진 기존의 핀시아 거버넌스 컨소시엄 체제는 지속된다. 핀시아 재단 관계자는 “기존 컨소시엄 체제는 그대로 운영되고 있다”면서 “라인은 핀시아의 주요 거버넌스 멤버 중 하나로 참가하고, 도시 등 라인 넥스트의 핀시아 기반 웹3 생태계 활성화를 위해 노력하고 있다”고 전했다. 라인과 핀시아 재단의 역할을 분명히 나누고, 핀시아를 기반으로 웹3 대중화를 추구하겠다는 전략이다. 핀시아 재단 관계자는 “핀시아는 퍼블릭 메인넷으로 사용자를 더욱 확보하고 이코노미를 확장하는 게 2024년 목표”라고 전했다.\n",
      "\n",
      "NFT 사업을 담당하고 있는 라인 넥스트는 도시에 더욱 힘을 줄 계획이다. 라인 관계자는 “2024년에는 NFT 마켓 도시를 정식 출시해 다양한 앱·게임·브랜드를 플랫폼에 온보딩할 계획”이라며 “새로운 소셜·게임 앱을 출시해 대중화 사례를 만들 것”이라고 밝혔다. 오랜 준비 끝에 실전에 돌입한 라인이 블록체인 사업에서 대중화를 이끌어낼지 업계의 관심도 이어질 전망이다.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://kr.investing.com/news/cryptocurrency-news/article-965681'\n",
    "remove_pattern = (r'읽기', r'provided', r'[a-z0-9]@[a-z0-9]', r'무단\\s*?전재 및 재배포',\n",
    "                  r'이 글의 독자는 인베스팅프로 결제 시 쿠폰\\s*?코드', r'인베스팅닷컴 유저를 위한', r'지금 인베스팅프로', r'인베스팅프로\\+를')\n",
    "remove_pattern = r'|'.join(remove_pattern)\n",
    "p0 = re.compile(pattern=r'\\s+\\r\\n\\s+')\n",
    "p1 = re.compile(pattern=r'\\n+')\n",
    "p2 = re.compile(pattern=rf'({remove_pattern})', flags=re.IGNORECASE)\n",
    "p3 = re.compile(pattern=r'(사진|출처)\\s*=')\n",
    "p4 = re.compile(pattern=r'\\n{3,}')\n",
    "# 일부 파라미터들을 고정한 investing 함수 생성\n",
    "fixed_investing = partial(investing,\n",
    "                            headers=headers, allow_redirects=allow_redirects, timeout=timeout, max_retry=max_retry,\n",
    "                            p0=p0, p1=p1, p2=p2, p3=p3, p4=p4)\n",
    "\n",
    "res = fixed_investing(url)\n",
    "print(res['news_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
